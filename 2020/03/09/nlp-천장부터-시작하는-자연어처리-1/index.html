<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!--Description-->
    
        <meta name="description" content="트랜스포머 네트워크
본 포스팅은 “한국어 임베딩” 책을 바탕으로 합니다.
트랜스포머(Vaswani et al., 2017) 네트워크는 구글 연구 팀이 NIPS에 공개한 딥러닝 아키텍처이다. 뛰어난 성능으로 주목받았다. 이후 발표된 GPT, BERT 등 기법은 트랜스포머">
    

    <!--Author-->
    
        <meta name="author" content="Sunny Cho">
    

    <!--Open Graph Title-->
    
        <meta property="og:title" content="[nlp] 천장부터 시작하는 자연어처리 (1)"/>
    

    <!--Open Graph Description-->
    

    <!--Open Graph Site Name-->
    <meta property="og:site_name" content="QuantPsy"/>

    <!--Type page-->
    
        <meta property="og:type" content="article" />
    

    <!--Page Cover-->
    

    <meta name="twitter:card" content="summary" />
    

    <!-- Title -->
    
    <title>[nlp] 천장부터 시작하는 자연어처리 (1) - QuantPsy</title>

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.2/css/bootstrap.min.css" integrity="sha384-y3tfxAZXuh4HwSYylfB+J125MxIs6mR5FOHamPBG064zB+AFeWH94NdvaCBm8qnd" crossorigin="anonymous">

    <!-- Custom Fonts -->
    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="//oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="//oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- Gallery -->
    <link href="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.css" type="text/css" rel="stylesheet" />

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/style.css">

    <!-- Google Analytics -->
    


</head>


<body>

<div class="bg-gradient"></div>
<div class="bg-pattern"></div>

<!-- Menu -->
<!--Menu Links and Overlay-->
<div class="menu-bg">
    <div class="menu-container">
        <ul>
            
            <li class="menu-item">
                <a href="/">
                    Home
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/archives">
                    Archives
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/about">
                    About
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/tags">
                    Tags
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/categories">
                    Categories
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/contact.html">
                    Contact
                </a>
            </li>
            
        </ul>
    </div>
</div>

<!--Hamburger Icon-->
<nav>
    <a href="#menu"></a>
</nav>

<div class="container">

    <!-- Main Content -->
    <div class="row">
    <div class="col-sm-12">

        <!--Title and Logo-->
        <header>
    <div class="logo">
        <a href="/"><i class="logo-icon fa fa-cube" aria-hidden="true"></i></a>
        
    </div>
</header>

        <section class="main">
            
<div class="post">

    <div class="post-header">
        <h1 class="title">
            <a href="/2020/03/09/nlp-%EC%B2%9C%EC%9E%A5%EB%B6%80%ED%84%B0-%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94-%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC-1/">
                [nlp] 천장부터 시작하는 자연어처리 (1)
            </a>
        </h1>
        <div class="post-info">
            
                <span class="date">2020-03-09</span>
            
            
            
                <span class="category">
                    <a href="/categories/nlp/">NLP</a>
                </span>
            
        </div>
    </div>

    <div class="content">

        <!-- Gallery -->
        

        <!-- Post Content -->
        <h1 id="트랜스포머-네트워크"><a href="#트랜스포머-네트워크" class="headerlink" title="트랜스포머 네트워크"></a>트랜스포머 네트워크</h1><p><br></p>
<h5 id="본-포스팅은-“한국어-임베딩”-책을-바탕으로-합니다"><a href="#본-포스팅은-“한국어-임베딩”-책을-바탕으로-합니다" class="headerlink" title="본 포스팅은 “한국어 임베딩” 책을 바탕으로 합니다."></a>본 포스팅은 “한국어 임베딩” 책을 바탕으로 합니다.</h5><p><br><hr><br></p>
<p>트랜스포머(Vaswani et al., 2017) 네트워크는 구글 연구 팀이 NIPS에 공개한 딥러닝 아키텍처이다. 뛰어난 성능으로 주목받았다. 이후 발표된 GPT, BERT 등 기법은 트랜스포머 블록을 기본 모델로 쓰고 있다. 이제부터 트랜스포머 모델 가운데 기본 블록의 산 과정과 그 작동 원리를 살펴보도록 한다. 하단 멀티헤드 어텐션과 상단 피드포워 네트워크를 설명한다.</p>
<p><br></p>
<h4 id="Scaled-Dot-Product-Attention"><a href="#Scaled-Dot-Product-Attention" class="headerlink" title="Scaled Dot-Product Attention"></a>Scaled Dot-Product Attention</h4><p>트랜스포머 블록의 주요 구성 요소 가운데 하나가 바로 Scaled Dot-Product Attention이다. 여기서 입력은 기본적으로 행렬 형태를 가지며 그 크기는 입력 문장의 단어 수 <em> 입력 임베딩의 차원 수다. 예컨대 “드디어, 금요일, 이다”라는 문장이 Scaled Dot-Product Attention 입력으로 들어가고 그 히든 차원 수가 768차원이라면 이 레이어의 력 행렬의 크기는 3 </em> 768이 된다.</p>
<p><br></p>
<p>트랜스포머의 Scaled Dot-Product Attention 메터니즘은 쿼리, 키, 값 세 가지 사이의 다이나믹스가 핵심이다. 입력 행렬과 쿼리, 키, 값에 해당하는 행렬을 각각 곱해 계한다. 이후 쿼리와 키가 얼마나 관련을 맺고 있는지를 나타내기 위해 입력 행렬과 쿼리 행렬을 곱한 결과와 입력 행렬과 키 행렬을 곱한 결과를 내적하여 키 벡터의 차원 수의 제곱근을 나누어준 뒤 소프트맥스를 취한다. 마지막으로 이 확률 값을 가중치 삼아 값에 해당하는 벡터들을 가중합한다.</p>
<p><br></p>
<p>Scaled Dot-Product Attention은 어떻게 단어들 사이의 의미적, 문법적 관계를 포착해낼 수 있는 걸까. 그 비결 중 일부는 기법 이름 속에 숨어 있다. 첫째는 dot-product, 둘째는 attention, 셋째는 scale이다. 선형대수학에서 dot-product는 내적을 달리 이르는 말이다. 내적은 벡터 간 유사도 측정 기법의 일종인 코사인 유사도와 깊은 관련 가진다. 두 벡터를 내적하고 이들 길이의 곱으로 나눠준 값이 바로 코사인 유사도다. 두 벡터가 모두 길이 1인 단위 벡터라면 두 벡터의 내적 자체가 코사인 유사도이며 그 값이 크면 클수록 코사인 유사도는 높아진다. 코사인 유사도는 두 벡터 사이의 각가 0도로 완전히 같을 경우 최대값 1, 180도로 완전히 반대인 경우 최소값 -1이 된다.</p>
<p><br></p>
<p>어떤 쿼리와 키가 특정 태스크 수행에 중요한 역할을 하고 있다면 트랜스포머 블록은 이들 사이의 내적 값을 키우는 방식으로 학습한다. 내적 값이 커지면 해당 쿼리와 키 벡터 공간상 가까이에 있을 가능성이 높아진다. 셀프 어텐션은 같은 문장 내 모든 어 쌍 사이의 의미적, 문법적 관계를 포착해낸다는 뜻이다. Scaled Dot-Production Attention은 소프트맥스 행렬과 값 행렬을 내적하는 것으로 마무리된다. 이는 소프트맥스 확률을 가중치 삼아 각 값 벡터들을 가중합하는 것과 같다. 따라서 Scaled Dot-Production Attention 결과 새롭게 만드어진 벡터는 해당 문장 내 단어 쌍 관계가 모두 축된 결과다.</p>
<p><br></p>
<p>셀프 어텐션은 기존 RNN, CNN 보다 장점이 많다. CNN의 경우 사용자가 정한 특정 윈도우 내의 로컬 문맥만 살핀다. 예컨대 문장 길이가 꽤 길고 맨 처음 단어와 마지막 단 사이의 연관성 파악이 태스크 수행에 중요한 데이터라면 CNN으로 이를 해결하기는 지 않다. 한편 RNN은 시퀀스 길이가 길어질수록 그래디언트 문제가 발생할 염려가 있다. 다시 말해 시퀀스가 길면 처음 입력받았던 단어는 RNN 모델이 잊어버릴 가능성이 높다는 이야기다. 하지만 트랜스포머와 같은 셀프 어텐션 기법은 문장 내 모든 단어쌍 사이의 관계를 늘 전체적으로 파악할 수 있다. 이 덕분에 트랜스포머는 컨볼루션 필나 RNN 셀 없이도 자연어 처리 태스크를 성공적으로 수행할 수 있었다.</p>
<p><br></p>
<p>또 다른 중요 요소는 scale이다. Scaled Dot-Product Attention에서는 쿼리와 키 행렬을 내적한 뒤 키 행렬 차원 수의 제곱근을 나눠줘 스케일을 하고 있다. Vaswani et al. (2017)에 따르면 이 경우에 쿼리-키 내적 행렬의 분산을 줄이게 돼 소프트맥스의 그래디언트가 지나치게 작아지는 것을 방지할 수 있다.</p>
<p><br><br></p>
<h4 id="멀티헤드-어텐션"><a href="#멀티헤드-어텐션" class="headerlink" title="멀티헤드 어텐션"></a>멀티헤드 어텐션</h4><p>트랜스포머 블록에서 멀티헤드 어텐션은 Scaled Dot-Product Attention을 여러 번 시하는 것을 가리킨다. 동일한 문장을 여러 명의 독자가 동시에 분석해 최선의 결과를 내려고 하는 것에 비유할 수 있겠다. 멀티헤드 어텐션의 계산 과정은 미리 만들어놓은 쿼리, 키, 값에 Scaled Dot-Production을 h번 수행한다. 이후 각 헤드의 결과 행렬을 이어 붙여 긴 행렬을 만들어 사용한다.</p>
<p><br><br></p>
<h4 id="Position-wise-Feedforward-Networks"><a href="#Position-wise-Feedforward-Networks" class="headerlink" title="Position-wise Feedforward Networks"></a>Position-wise Feedforward Networks</h4><p>멀티헤드 어텐션 레이어의 입력 행렬과 출력 행렬의 크기는 입력 단어 수 * 히든 벡터 차원 수로 동일하다. Position-wise Feedforward Networks 레이어에서는 멀티헤드 어텐션 레이어의 출력 행렬을 행 벡터 단위로, 다시 말해 단어 벡터 각각에 관해 수식을 적용한다. 멀티헤드 어텐션 레이어의 출력 행렬 가운데 하나의 단어 벡터에 관해 두 번의 선형변환을 수행하며 그 사이에 ReLU를 적용한다.</p>
<p><br><br></p>
<h4 id="트랜스포머의-학습-전략"><a href="#트랜스포머의-학습-전략" class="headerlink" title="트랜스포머의 학습 전략"></a>트랜스포머의 학습 전략</h4><p>트랜스포머의 학습 전략은 웜업(warm up)이다. 사용자가 정한 스텝 수에 이르기까지 습률을 올렸다가 스텝 수를 만족하면 조금씩 떨어뜨리는 방식이다. 대규모 데이터,  모델 학습에 적합하다. 이 전략은 BERT 등 이후 제안된 모델에도 널리 쓰이고 있다. 이밖에 레이어 정규화 등도 트랜스포머의 안정적인 학습에 기여하고 있는 것으로 보다.</p>
<p><br><br></p>
<h4 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h4><p>BERT(Bidirectional Encoder Representations from Transformer)는 구글에서 공개한 모델이다. 성능이 뛰어나 널리 쓰이고 있다. BERT의 성공 비결은 그 퍼포먼스가 검증된 트랜스포머 블록을 썼을뿐더러 모델의 속성이 양방향을 지향한다는 점에 있다. GPT는 단어 시퀀스를 왼쪽에서 오른쪽으로 한 방향으로만 모든 아키텍처이다. ELMo는 Bi-LSTM 레이어의 상단은 양방향이지만 중간 레이어 역시 한 방향인 모델이다. 반면 BERT의 경우 모든 레이어에서 양방향 성질을 잃지 않고 있다.</p>
<p><br></p>
<p>BERT와 GPT 모델은 모두 트랜스포머 블록을 사용하고 있다. 그렇다면 GPT는 왜 단어들을 양방향으로 보지 못하는 것일까? 그 이유는 GPT가 언어 모델이기 때문이다. GPT는 주어진 단어 시퀀스를 가지고 그다음 단어를 예측하는 과정에서 학습한다. 이 경우 현재 입력 단어 이후의 단어를 모델에게 알려주는 것은 반칙이다. 언어 모델은 주어진 시퀀스를 가지고 다음 단어를 맞춰야 하는데, 맞춰야할 정답을 미리 알려줄 수는 없기 때문이다. Devline et al. (2018)은 이 문제를 극복하기 위해 마스크 언어 모델(masked language model)을 제안했다. 주어진 시퀀스 다음 단어를 맞추는 것에서 벗어나, 일단 문장 전체를 모델에게 알려주고, 빈칸(MASK)에 해당하는 단어가 어떤 단어일지 예측하는 과정에서 학습을 해보자는 아이디어다. 마스크 언어 모델 태스크에서는 모델에 문장 전체를 다 주어도 반칙이 될 수 없다. BERT 모델은 빈칸을 채워야 하기 때문이다.</p>
<p><br></p>
<p>GPT는 Scaled Dot-Product Attention을 하는 과정에서 예측해야 할 단어를 보지 않기 위해 소프트맥스 스코어 행렬의 일부 값을 0으로 만든다. 예컨대 입력 문장이 꿈, 보다, 해몽이고 이번에 예측해야 할 단어가 보다라면 GPT는 이전 단어인 꿈만 참고할 수 있다. 해몽이라는 단어를 맞춰야 할 순서라면 GPT는 꿈, 보다만을 참고해야 한다. 반면 BERT는 빈칸만 맞추면 되기 때문에 그림처럼 문장 내 단어 쌍 사이의 관계를 모두 볼 수 있다.</p>
<p><br></p>
<p>Devlin et al. (2018)은 BERT 임베딩을 각종 다운스트림 태스크에 적용해 실험한 결과 BERT의 임베딩 품질이 GPT보다 좋음을 입증했다. 그만큼 모델이 양방향 전후 문맥을 모두 보게 하는 것이 중요하다는 이야기다. BERT가 양방향 모델이 될 수 있었던 배경은 모델이 순방향, 역방향 문맥을 모두 볼 수 있도록 하는 학습 태스크 덕분이다. BERT의 프리트레인 태스크에는 크게 마스크 언어 모델, 다음 문장인지 여부 맞추기 두 가지가 있다. 먼저 마스크 언어 모델 태스크 수행을 위한 학습 데이터는 다음과 같이 만든다.</p>
<p><br></p>
<ul>
<li>학습 데이터 한 문장 토큰의 15%를 마스킹한다.</li>
<li>마스킹 대상 토큰 가운데 80%는 실제 빈칸으로 만들고, 모델은 그 빈칸을 채운다.</li>
<li>마스킹 대상 토큰 가운데 10%는 랜덤으로 다른 토큰으로 대체하고, 모델은 해당 위치의 정답 언어가 무엇일지 맞추도록 한다.</li>
<li>마스킹 대상 토큰 가운데 10%는 토큰 그대로 두고, 모델은 해당 위치의 정답 단어가 무엇일지 맞추도록 한다.</li>
</ul>
<p><br></p>

    </div>

    

    
        <div class="post-tags">
            <i class="fa fa-tags" aria-hidden="true"></i>
            <a href="/tags/nlp/">#NLP</a>
        </div>
    

    <!-- Comments -->
    

</div>
        </section>

    </div>
</div>


</div>

<!-- Footer -->
<div class="push"></div>

<footer class="footer-content">
    <div class="container">
        <div class="row">
            <div class="col-xs-12 col-sm-12 col-md-6 col-lg-6 footer-about">
                <h2>About</h2>
                <p>
                    This theme was developed by <a href="https://github.com/klugjo" target="_blank" rel="noopener">Jonathan Klughertz</a>. The source code is available on Github. Create Websites. Make Magic.
                </p>
            </div>
            
    <div class="col-xs-6 col-sm-6 col-md-3 col-lg-3 recent-posts">
        <h2>Recent Posts</h2>
        <ul>
            
            <li>
                <a class="footer-post" href="/2020/04/15/ios-%EC%95%B1-%EA%B0%9C%EB%B0%9C-%EC%8B%A4%EC%8A%B5%ED%8E%B8-1/">[iOS] 앱 개발 실습편 (1)</a>
            </li>
            
            <li>
                <a class="footer-post" href="/2020/04/13/algorithm-%EA%B7%B8%EB%9E%98%ED%94%84/">[Algorithm] 그래프</a>
            </li>
            
            <li>
                <a class="footer-post" href="/2020/04/09/beakjoon-1-7/">[BaekJoon] A/B</a>
            </li>
            
            <li>
                <a class="footer-post" href="/2020/04/04/SEM-%EC%B8%A1%EC%A0%95%EB%AA%A8%ED%98%95/">[SEM] 측정모형</a>
            </li>
            
        </ul>
    </div>



            
<div class="col-xs-6 col-sm-6 col-md-3 col-lg-3 footer-categories">
    <h2>Categories</h2>
    <ul>
        
        <li>
            <a class="footer-post" href="/categories/hci/">HCI</a>
        </li>
        
        <li>
            <a class="footer-post" href="/categories/nlp/">NLP</a>
        </li>
        
        <li>
            <a class="footer-post" href="/categories/algorithm/selection-sort/">selection sort</a>
        </li>
        
        <li>
            <a class="footer-post" href="/categories/algorithm/graph/">graph</a>
        </li>
        
    </ul>
</div>

        </div>
        <div class="row">
            <div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">
                <ul class="list-inline footer-social-icons">
                    
                    <li class="list-inline-item">
                        <a href="https://github.com/klugjo/hexo-theme-alpha-dust" target="_blank" rel="noopener">
                            <span class="footer-icon-container">
                                <i class="fa fa-github"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="https://twitter.com/?lang=en" target="_blank" rel="noopener">
                            <span class="footer-icon-container">
                                <i class="fa fa-twitter"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="https://www.facebook.com/" target="_blank" rel="noopener">
                            <span class="footer-icon-container">
                                <i class="fa fa-facebook"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="https://www.instagram.com/" target="_blank" rel="noopener">
                            <span class="footer-icon-container">
                                <i class="fa fa-instagram"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="https://dribbble.com/" target="_blank" rel="noopener">
                            <span class="footer-icon-container">
                                <i class="fa fa-dribbble"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="https://plus.google.com/" target="_blank" rel="noopener">
                            <span class="footer-icon-container">
                                <i class="fa fa-google-plus"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="https://www.behance.net/" target="_blank" rel="noopener">
                            <span class="footer-icon-container">
                                <i class="fa fa-behance"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="https://500px.com/" target="_blank" rel="noopener">
                            <span class="footer-icon-container">
                                <i class="fa fa-500px"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="mailto:test@example.com" target="_blank" rel="noopener">
                            <span class="footer-icon-container">
                                <i class="fa fa-envelope-o"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="\#">
                            <span class="footer-icon-container">
                                <i class="fa fa-rss"></i>
                            </span>
                        </a>
                    </li>
                    
                </ul>
            </div>
        </div>
        <div class="row">
            <div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">
                <div class="footer-copyright">
                    @Untitled. All right reserved | Design & Hexo <a href="http://www.codeblocq.com/" target="_blank" rel="noopener">Jonathan Klughertz</a>
                </div>
            </div>
        </div>
    </div>
</footer>

<!-- After footer scripts -->

<!-- jQuery -->
<script src="//code.jquery.com/jquery-2.1.4.min.js"></script>

<!-- Tween Max -->
<script src="//cdnjs.cloudflare.com/ajax/libs/gsap/1.18.5/TweenMax.min.js"></script>

<!-- Gallery -->
<script src="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.js" type="text/javascript" charset="utf-8"></script>

<!-- Custom JavaScript -->
<script src="/js/main.js"></script>

<!-- Disqus Comments -->



<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML'></script>

</body>

</html>