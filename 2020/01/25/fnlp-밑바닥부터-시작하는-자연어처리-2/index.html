<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!--Description-->
    
        <meta name="description" content="밑바닥부터 시작하는 자연어처리 (2)본 포스팅은 “한국어 임베딩(이기창)”을 바탕으로 “텍스트 마이닝 실전 및 분석(송민)” 강좌를 추가하여 정리한 내용입니다.
한국어 임베딩

02 벡터가 어떻게 의미를 가지게 되는가

2.1 자연어 계산과 이해2.2 어떤 단어가 많이">
    

    <!--Author-->
    
        <meta name="author" content="Sunny Cho">
    

    <!--Open Graph Title-->
    
        <meta property="og:title" content="[fnlp] 밑바닥부터 시작하는 자연어처리 (2)"/>
    

    <!--Open Graph Description-->
    

    <!--Open Graph Site Name-->
    <meta property="og:site_name" content="QuantPsy"/>

    <!--Type page-->
    
        <meta property="og:type" content="article" />
    

    <!--Page Cover-->
    

    <meta name="twitter:card" content="summary" />
    

    <!-- Title -->
    
    <title>[fnlp] 밑바닥부터 시작하는 자연어처리 (2) - QuantPsy</title>

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.2/css/bootstrap.min.css" integrity="sha384-y3tfxAZXuh4HwSYylfB+J125MxIs6mR5FOHamPBG064zB+AFeWH94NdvaCBm8qnd" crossorigin="anonymous">

    <!-- Custom Fonts -->
    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="//oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="//oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- Gallery -->
    <link href="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.css" type="text/css" rel="stylesheet" />

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/style.css">

    <!-- Google Analytics -->
    


</head>


<body>

<div class="bg-gradient"></div>
<div class="bg-pattern"></div>

<!-- Menu -->
<!--Menu Links and Overlay-->
<div class="menu-bg">
    <div class="menu-container">
        <ul>
            
            <li class="menu-item">
                <a href="/">
                    Home
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/archives">
                    Archives
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/about">
                    About
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/tags">
                    Tags
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/categories">
                    Categories
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/contact.html">
                    Contact
                </a>
            </li>
            
        </ul>
    </div>
</div>

<!--Hamburger Icon-->
<nav>
    <a href="#menu"></a>
</nav>

<div class="container">

    <!-- Main Content -->
    <div class="row">
    <div class="col-sm-12">

        <!--Title and Logo-->
        <header>
    <div class="logo">
        <a href="/"><i class="logo-icon fa fa-cube" aria-hidden="true"></i></a>
        
    </div>
</header>

        <section class="main">
            
<div class="post">

    <div class="post-header">
        <h1 class="title">
            <a href="/2020/01/25/fnlp-%EB%B0%91%EB%B0%94%EB%8B%A5%EB%B6%80%ED%84%B0-%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94-%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC-2/">
                [fnlp] 밑바닥부터 시작하는 자연어처리 (2)
            </a>
        </h1>
        <div class="post-info">
            
                <span class="date">2020-01-25</span>
            
            
            
                <span class="category">
                    <a href="/categories/natural-language-processing/">Natural Language Processing</a>
                </span>
            
        </div>
    </div>

    <div class="content">

        <!-- Gallery -->
        

        <!-- Post Content -->
        <h1 id="밑바닥부터-시작하는-자연어처리-2"><a href="#밑바닥부터-시작하는-자연어처리-2" class="headerlink" title="밑바닥부터 시작하는 자연어처리 (2)"></a>밑바닥부터 시작하는 자연어처리 (2)</h1><h6 id="본-포스팅은-“한국어-임베딩-이기창-”을-바탕으로-“텍스트-마이닝-실전-및-분석-송민-”-강좌를-추가하여-정리한-내용입니다"><a href="#본-포스팅은-“한국어-임베딩-이기창-”을-바탕으로-“텍스트-마이닝-실전-및-분석-송민-”-강좌를-추가하여-정리한-내용입니다" class="headerlink" title="본 포스팅은 “한국어 임베딩(이기창)”을 바탕으로 “텍스트 마이닝 실전 및 분석(송민)” 강좌를 추가하여 정리한 내용입니다."></a>본 포스팅은 “한국어 임베딩(이기창)”을 바탕으로 “텍스트 마이닝 실전 및 분석(송민)” 강좌를 추가하여 정리한 내용입니다.</h6><p><br></br></p>
<h4 id="한국어-임베딩"><a href="#한국어-임베딩" class="headerlink" title="한국어 임베딩"></a>한국어 임베딩</h4><p></p>

<h5 id="02-벡터가-어떻게-의미를-가지게-되는가"><a href="#02-벡터가-어떻게-의미를-가지게-되는가" class="headerlink" title="02 벡터가 어떻게 의미를 가지게 되는가"></a>02 벡터가 어떻게 의미를 가지게 되는가</h5><p></p>

<p>2.1 자연어 계산과 이해<br>2.2 어떤 단어가 많이 쓰였는가</p>
<ul>
<li>백오브워즈 가정</li>
<li>TF-IDF</li>
<li>Deep Averaging Network<br>2.3 단어가 어떤 순서로 쓰였는가</li>
<li>통계 기반 언어 모델</li>
<li>뉴럴 네트워크 기반 언어 모델<br>2.4 어떤 단어가 같이 쓰였는가</li>
<li>분포 가정</li>
<li>분포의 의미(1): 형태소</li>
<li>분포의 의미(2): 품사</li>
<li>점별 상호 정보량</li>
<li>Word2Vec</li>
</ul>
<p><br></br></p>
<h5 id="제7강-벡터-공간-모델-1"><a href="#제7강-벡터-공간-모델-1" class="headerlink" title="제7강 벡터 공간 모델 1"></a>제7강 벡터 공간 모델 1</h5><p></p>

<ol>
<li>벡터 공간 모델</li>
<li>문헌-단어 매트릭스 생성</li>
<li>단어 가중치 기법</li>
</ol>
<p><br></br></p>
<p><hr><br><br></br></p>
<h4 id="자연어-계산과-이해"><a href="#자연어-계산과-이해" class="headerlink" title="자연어 계산과 이해"></a>자연어 계산과 이해</h4><p></p>

<p>컴퓨터는 자연어를 사람처럼 이해할 수 없다. 컴퓨터는 그저 계산기일 뿐이다. 그런데 임베딩을 활용하면 컴퓨터가 자연어를 계산하는 것이 가능해진다. 임베딩은 자연어를 컴퓨터가 처리할 수 잇는 숫자들의 나열인 벡터로 바꾼 결과이기 때문이다. 컴퓨터는 임베딩을 계산/처리해 사람이 알아들을 수 있는 형태의 자연어로 출력한다. 사람 말을 100% 이해하는 인공지능이 등장하더라도 그 이해의 본질은 계산이다.</p>
<p></p>

<p>그러면 임베딩에 자연어 의미를 어떻게 함축할 수 있을까. 그 비결은 자연어의 통계적 패턴 정보를 통째로 임베딩에 넣는 것이다. 자연어의 의미는 해당 언어 화자들이 실제 사용하는 일상 언어에서 드러나기 때문이다. 임베딩을 만들 때 쓰는 통계 정보는 크게 세 가지가 있다. 첫째는 문장에 어떤 단어가 (많이) 쓰였는지이고, 둘째는 단어가 어떤 순서로 등장하는지이며, 마지막으로는 문장에 어떤 단어가 같이 나타났는지와 관련한 정보다.</p>
<p><br></br></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>구분</th>
<th style="text-align:center">백오브워즈 가정</th>
<th style="text-align:center">언어 모델</th>
<th style="text-align:right">분포 가정</th>
</tr>
</thead>
<tbody>
<tr>
<td>내용</td>
<td style="text-align:center">어떤 단어가 (많이) 쓰였는가</td>
<td style="text-align:center">단어가 어떤 순서로 쓰였는가</td>
<td style="text-align:right">어떤 단어가 같이 쓰였는가</td>
</tr>
<tr>
<td>대표 통계량</td>
<td style="text-align:center">TF-IDF</td>
<td style="text-align:center">-</td>
<td style="text-align:right">PMI</td>
</tr>
<tr>
<td>대표 모델</td>
<td style="text-align:center">Deep Averaging Network</td>
<td style="text-align:center">ELMo, GPT</td>
<td style="text-align:right">Word2Vec</td>
</tr>
</tbody>
</table>
</div>
<p><br></br></p>
<p>백오브워즈(bag of words) 가정에서는 어떤 단어가 (많이) 쓰였는지 정보를 중시한다. 저자의 의도는 단어 사용 여부나 그 빈도에서 드러난다고 보기 때문이다. 단어의 순서(order) 정보는 무시한다. 백오브워즈 가정에서 가장 많이 쓰이는 통계량은 Term Frequency-Inverse Document Frequency이며, 백오브워즈 가정의 딥러닝 버전은 Deep Averaging Network(lyyer et al. 2015)다.</p>
<p></p>

<p>단어의 등장 순서를 무시하는 백오브워즈 가정의 대척점에는 언어 모델(language model)이 있다. 언어 모델은 단어의 등장 순서를 학습해 주어진 단어 시퀀스가 얼마나 자연스러운지 확률을 부여한다. ELMo, GPT 등과 같은 뉴럴 네트워크 기반의 언어 모델이 여기에 해당한다.</p>
<p></p>

<p>분포 가정(distributional hypothesis)에서는 문장에서 어떤 단어가 같이 쓰였는지를 중요하게 따진다. 단어의 의미는 그 주변 문맥(context)을 통해 유추해볼 수 있다고 보는 것이다. 분포 가정의 대표 통계량은 점별 상호 정보량(PMI, Pointwise Mutual Information)이며 대표 모델은 Word2Vec을 꼽을 수 있다.</p>
<p></p>

<p>위의 세 철학은 서로 연관이 있다. 언어 모델에서는 단어의 등장 순서를, 분포 가정에서는 이웃 단어(문맥)을 우선시한다. 어떤 단어가 문장에서 주로 나타나는 순서는 해당 단어의 주변 문맥과 떼려야 뗼 수 없는 관계를 가진다. 한편 분포 가정에서는 어떤 단어 쌍이 얼마나 같이 자주 나타나는지와 관련한 정보를 수치화하기 위해 개별 단어 그리고 단어 쌍의 빈도 정보를 적극 활용한다. 예컨대 백오브워즈 가정, 언어 모델, 분포 가정은 말뭉치의 통계적 패턴을 서로 다른 각도에서 분석하는 것이며 상호 보완적이다.</p>
<p><br></br><br></p>
<h4 id="어떤-단어가-많이-쓰였는가"><a href="#어떤-단어가-많이-쓰였는가" class="headerlink" title="어떤 단어가 많이 쓰였는가"></a>어떤 단어가 많이 쓰였는가</h4><p><br></br></p>
<h5 id="백오브워즈-가정"><a href="#백오브워즈-가정" class="headerlink" title="백오브워즈 가정"></a>백오브워즈 가정</h5><p></p>

<p>수학에서 백(bag)이란 중복 원소를 허용한 집합(multiset)을 뜻한다. 원소의 순서는 고려하지 않는다. 자연어 처리 분야에엇 백오브워즈란 단어의 등장 순서에 관계없이 문서 내 단어의 등장 빈도를 임베딩으로 쓰는 기법을 말한다. 백오브워즈는 문장을 단어들로 나누고 이들을 중복집합에 넣어 임베딩으로 활용하는 것이라고 보면 된다. 아래 예시처럼 단어들의 빈도를 세어 놓은 것으로 이해하면 쉽다. 경우에 따라서는 빈도 역시 단순화해 등장 여부(등장 시 1, 아니면 0)만을 백오브워즈 임베딩으로 쓰기도 한다.</p>
<p></p>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 텍스트 자료</span></span><br><span class="line">별 하나 에 추억 과</span><br><span class="line">별 하나 에 사랑 과</span><br><span class="line">별 하나 에 씁쓸함 과</span><br><span class="line">별 하나 에 동경 과</span><br><span class="line">별 하나 에 시 와</span><br><span class="line">별 하나 에 어머니, 어머니</span><br><span class="line"></span><br><span class="line"><span class="comment"># 백오브워즈 임베딩</span></span><br><span class="line"> 별 하나 에 추억 과 사랑 씁쓸 함 ... .</span><br><span class="line">[<span class="number">6</span>,  <span class="number">6</span>,  <span class="number">6</span>,  <span class="number">1</span>,  <span class="number">4</span>,  <span class="number">1</span>,  <span class="number">1</span>,  <span class="number">1</span>, ..., <span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<p></p>

<p>백오브워즈 임베딩에는 ‘저자가 생각한 주제가 문서에서의 단어 사용에 녹아 있다’는 가정이 깔려 있다. 다시 말해 주제가 비슷한 문서라면 단어 빈도 또는 단어 등장 여부 역시 비슷할 것이고, 백오브워즈 임베딩 역시 유사할 것이라고 보는 것이다. 빈도를 그대로 백오브워즈로 쓴다면 많이 쓰인 단어가 주제와 더 강한 관련을 맺고 있을 것이라는 전제 역시 깔려 있다. </p>
<p></p>

<p>백오브워즈 임베딩은 간단한 아이디어지만 정보 검색 분야에서 여전히 많이 쓰이고 있다. 사용자의 질의에 가장 적절한 문서를 보여줄 때 질의를 백오브워즈 임베딩으로 변환하고 질의와 검색 대상 문서 임베딩 간 코사인 유사도를 구해 유사도가 가장 높은 문서를 사용자에게 노출한다.</p>
<p><br></br><br></p>
<h4 id="TF-IDF"><a href="#TF-IDF" class="headerlink" title="TF-IDF"></a>TF-IDF</h4><p>단어 빈도 또는 등장 여부를 그대로 임베딩으로 쓰는 것에는 큰 단점이 있다. 어떤 문서에든 쓰여서 해당 단어가 (많이) 나타났다 하더라도 문서의 주제를 가늠하기 어려운 경우가 있기 때문이다. 예컨대 ‘을/를’, ‘이/가’ 같은 조사는 대부분의 한국어 문서에서 등장한다. 이에 우리는 ‘을/를’ 또는 ‘이/가’만으로는 해당 문서의 주제를 추측하기 어렵다.</p>
<p></p>

<p>이러한 단점을 보완하기 위해 제안된 기법이 Term Frequency-Inverse Document Frequency이다. 단어-문서 행렬에 아래 수식과 같이 가중치를 계산해 행렬 원소를 바꾼다. TF-IDF 역시 단어 등장 순서를 고려하지 않는다는 점에서 백오브워즈 임베딩이라고 이해할 수 있다.</p>
<p><br></br></p>
<script type="math/tex; mode=display">TF-IDF(w) = TF(w) \times log(\frac{N}{DF(w)})</script><p><br></br></p>
<p>수식에서 TF(Term Frequency)는 어떤 단어가 특정 문서에 얼마나 많이 쓰였는지 빈도를 나타낸다. 많이 쓰인 단어가 중요하다는 가정을 전제로 한 수치다. 예컨대 A라는 단어가 문서 1에서 10번, 문서 3에서 5번 쓰였다면 문서 1의 단어 A의 TF는 10, 문서 3의 단어 A의 TF는 5가 된다.</p>
<p></p>

<p>DF(Document Frequency)란 특정 단어가 나타난 문서의 수를 뜻한다. 만약 A라는 단어가 말뭉치 전체에서 문서 1, 문서 3에만 등장했다면 DF는 2가 된다. DF가 클수록 다수 문서에 쓰이는 범용적인 단어라고 볼 수 있겠다. TF는 같은 단어라도 문서마다 다른 값을 갖고, DF는 문서가 달라지더라도 단어가 같다면 동일한 값을 지닌다.</p>
<p></p>

<p>IDF(Inverse Document Frequency)는 전체 문서 수(N)를 해당 단어의 DF로 나눈 뒤 로그를 취한 값이다. 그 값이 클수록 특이한 단어라는 뜻이다. 이는 단어의 주제 예측 능력(해당 단어만 보고 문서의 주제를 가늠해볼 수 있는 정도)과 직결된다.</p>
<p></p>

<p>TF-IDF가 지향하는 원리는 이렇다. 어떤 단어의 주제 예측 능력이 강할 수록 가중치가 커지고 그 반대의 경우 작아진다. 한편 어떤 단어의 TF가 높으면 TF-IDF 값 역시 커진다. 단어 사용 빈도는 저자가 상정한 주제와 관련을 맺고 있을 것이라는 가정에 기초한 것이다.</p>
<p><br></br><br></p>
<h4 id="벡터-공간-모델"><a href="#벡터-공간-모델" class="headerlink" title="벡터 공간 모델"></a>벡터 공간 모델</h4><ul>
<li>텍스트 문서를 색인어와 같은 식별자(단어)들을 벡터로 나타내는 대수적인 모델</li>
<li>문서는 벡터로 표현되며, 각각의 차원은 개별 단어에 대응됨</li>
<li>만약 문서 내에 특정 단어가 포함되어 있다면, 벡터 내에서 해당 차원은 0이 아닌 값을 갖게 됨</li>
</ul>
<p><br></br></p>
<p>단순 벡터 공간 모델에 있어서는 단순 빈도, 즉 단어의 출현 빈도를 가지고 그 문헌을 표시하게 된다. 이를 통해 벡터 공간 모델을 만드는데, 이보다 더 의미 있는 분석을 하기 위해서는 단순 출현 빈도가 아닌 단어의 가중치를 이용하여 분할을 표현하게 되고, 벡터 공간 모델을 분석하게 된다. 가장 많이 쓰이는 단어 가중치 기법은 TF-IDF 값을 사용하는 것이다.</p>
<p></p>

<p>단어는 하나의 단어이거나 키워드 혹은 더 긴 구가 될 수 있다. 만약 단어가 단일어로 선택된다면, 벡터의 차원 수(Dimensionality)는 단어집(Vocabulary) 내의 단어들의 숫자(언어 자료 내에 들어 있는 개별 단어들의 숫자)가 된다. 즉 각각의 단어는 자질이 되고, 차원이 되어 이것들의 합은 단어집이 된다. 그리고 이 단어들의 숫자는 전체 문헌 집단에서 나타난 유일한(Unique) 단어의 집단의 합으로 단어집의 크기가 된다. </p>
<p></p>

<p>이러한 벡터 공간 모델을 만들기 위해서는 문서를 벡터화 해야 한다. 이때 문서를 벡터화 하는 작업이 굉장히 중요하다. 만약 정보 검색이라고 하면 정보 검색에 있어서는 질의어가 들어오면 질의어도 벡터화 한다. 이를 질의어 벡터라고 부를 수 있다. 그 다음에 색인된 문헌들, 즉 각각의 문헌들은 문헌 벡터가 된다. 그래서 질의어 벡터와 문헌 벡터 사이의 유사도를 구하여 각 유사도의 값이 높은 순서대로 정렬해서 이용자에게 그 검색 결과를 보여주는 것이 바로 벡터 공간 모델을 이용한 검색엔진이 된다.</p>
<p><br></br></p>
<ul>
<li>검색의 대상이 되는 문서 : $D_1, D_2, …, D_n$</li>
<li>문서 집합 전체에 걸친 총 m개의 색인어 : $w_1, w_2, …, w_m$</li>
<li>색인어 $w_i$의 문서 $d_j$에서의 가중치 :</li>
</ul>
<p></p>

<script type="math/tex; mode=display">d_j = \pmatrix{
d_{1j} \cr
d_{2j} \cr
\vdots \cr
d_{mj} \cr
}</script><p></p>

<ul>
<li>문서 집합 전체를 문헌*단어 행렬로 표현</li>
</ul>
<p></p>

<script type="math/tex; mode=display">D = \pmatrix{d_1 & d_2 & \ldots & d_n} = \pmatrix{
d_{11} & d_{12} & \ldots & d_{1n} \cr
d_{21} & d_{22} & \ldots & d_{2n} \cr
\vdots & \vdots & \ddots & \vdots \cr
d_{m1} & d_{m2} & \ldots & d_{mn} \cr
}</script><p></p>

<p>벡터 공간 모델에서 긴 문서들은 유사도 값이 작기 때문에 제대로 표현되지 않는다. 비슷한 의미를 갖고 있는 문서들일지라도 사용된 단어가 다르면 연관성을 갖지 못한다. 즉, 문헌 공간 모델을 만드는 데 있어서 각각의 단어는 독립된 단어라고 보기 때문에 비슷한 의미를 가진 단어라 하더라도 그것이 다른 독립된 단어로 잡히고 이럴 때 분석의 퀄리티가 떨어지게 된다. 또한 벡터 공간 표현에서는 단어들이 나타나는 순서가 무시되기 때문에 문맥 정보를 파악할 수 없다.</p>
<p></p>

<p><br></br></p>

    </div>

    

    
        <div class="post-tags">
            <i class="fa fa-tags" aria-hidden="true"></i>
            <a href="/tags/natural-language-processing/">#Natural Language Processing</a>
        </div>
    

    <!-- Comments -->
    

</div>
        </section>

    </div>
</div>


</div>

<!-- Footer -->
<div class="push"></div>

<footer class="footer-content">
    <div class="container">
        <div class="row">
            <div class="col-xs-12 col-sm-12 col-md-6 col-lg-6 footer-about">
                <h2>About</h2>
                <p>
                    This theme was developed by <a href="https://github.com/klugjo" target="_blank" rel="noopener">Jonathan Klughertz</a>. The source code is available on Github. Create Websites. Make Magic.
                </p>
            </div>
            
    <div class="col-xs-6 col-sm-6 col-md-3 col-lg-3 recent-posts">
        <h2>Recent Posts</h2>
        <ul>
            
            <li>
                <a class="footer-post" href="/2021/03/05/kaldi-gstreamer-server/">[STT] Kaldi Gstreamer Ser</a>
            </li>
            
            <li>
                <a class="footer-post" href="/2020/08/09/baekjoon-1-5/">[BaekJoon] 입출력과 사칙연산</a>
            </li>
            
            <li>
                <a class="footer-post" href="/2020/07/06/hackerrank-2d-arrays/">[HackerRank] 2D Arrays</a>
            </li>
            
            <li>
                <a class="footer-post" href="/2020/07/05/hackerrank-binary-numbers/">[HackerRank] Binary Nnumb</a>
            </li>
            
        </ul>
    </div>



            
<div class="col-xs-6 col-sm-6 col-md-3 col-lg-3 footer-categories">
    <h2>Categories</h2>
    <ul>
        
        <li>
            <a class="footer-post" href="/categories/hci/">HCI</a>
        </li>
        
        <li>
            <a class="footer-post" href="/categories/nlp/">NLP</a>
        </li>
        
        <li>
            <a class="footer-post" href="/categories/algorithm/selection-sort/">selection sort</a>
        </li>
        
        <li>
            <a class="footer-post" href="/categories/algorithm/graph/">graph</a>
        </li>
        
    </ul>
</div>

        </div>
        <div class="row">
            <div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">
                <ul class="list-inline footer-social-icons">
                    
                    <li class="list-inline-item">
                        <a href="https://github.com/klugjo/hexo-theme-alpha-dust" target="_blank" rel="noopener">
                            <span class="footer-icon-container">
                                <i class="fa fa-github"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="https://twitter.com/?lang=en" target="_blank" rel="noopener">
                            <span class="footer-icon-container">
                                <i class="fa fa-twitter"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="https://www.facebook.com/" target="_blank" rel="noopener">
                            <span class="footer-icon-container">
                                <i class="fa fa-facebook"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="https://www.instagram.com/" target="_blank" rel="noopener">
                            <span class="footer-icon-container">
                                <i class="fa fa-instagram"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="https://dribbble.com/" target="_blank" rel="noopener">
                            <span class="footer-icon-container">
                                <i class="fa fa-dribbble"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="https://plus.google.com/" target="_blank" rel="noopener">
                            <span class="footer-icon-container">
                                <i class="fa fa-google-plus"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="https://www.behance.net/" target="_blank" rel="noopener">
                            <span class="footer-icon-container">
                                <i class="fa fa-behance"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="https://500px.com/" target="_blank" rel="noopener">
                            <span class="footer-icon-container">
                                <i class="fa fa-500px"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="mailto:test@example.com" target="_blank" rel="noopener">
                            <span class="footer-icon-container">
                                <i class="fa fa-envelope-o"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="\#">
                            <span class="footer-icon-container">
                                <i class="fa fa-rss"></i>
                            </span>
                        </a>
                    </li>
                    
                </ul>
            </div>
        </div>
        <div class="row">
            <div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">
                <div class="footer-copyright">
                    @Untitled. All right reserved | Design & Hexo <a href="http://www.codeblocq.com/" target="_blank" rel="noopener">Jonathan Klughertz</a>
                </div>
            </div>
        </div>
    </div>
</footer>

<!-- After footer scripts -->

<!-- jQuery -->
<script src="//code.jquery.com/jquery-2.1.4.min.js"></script>

<!-- Tween Max -->
<script src="//cdnjs.cloudflare.com/ajax/libs/gsap/1.18.5/TweenMax.min.js"></script>

<!-- Gallery -->
<script src="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.js" type="text/javascript" charset="utf-8"></script>

<!-- Custom JavaScript -->
<script src="/js/main.js"></script>

<!-- Disqus Comments -->



<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML'></script>

</body>

</html>