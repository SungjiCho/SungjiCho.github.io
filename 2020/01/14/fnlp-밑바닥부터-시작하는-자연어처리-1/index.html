<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!--Description-->
    
        <meta name="description" content="한국어 임베딩: 서론본 포스팅은 “한국어 임베딩(이기창)”을 바탕으로 “밑바닥부터 시작하는 딥러 2(사이토 고키)”를 추가하여 정리한 내용입니다.
한국어 임베딩

01 서론

1.1 임베딩이란1.2 임베딩의 역할

단어/문장 간 관련도 계산
의미/문법 정보 함축
전이 ">
    

    <!--Author-->
    
        <meta name="author" content="Sunny Cho">
    

    <!--Open Graph Title-->
    
        <meta property="og:title" content="[FNLP] 밑바닥부터 시작하는 자연어처리 (1)"/>
    

    <!--Open Graph Description-->
    

    <!--Open Graph Site Name-->
    <meta property="og:site_name" content="QuantPsy"/>

    <!--Type page-->
    
        <meta property="og:type" content="article" />
    

    <!--Page Cover-->
    

    <meta name="twitter:card" content="summary" />
    

    <!-- Title -->
    
    <title>[FNLP] 밑바닥부터 시작하는 자연어처리 (1) - QuantPsy</title>

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.2/css/bootstrap.min.css" integrity="sha384-y3tfxAZXuh4HwSYylfB+J125MxIs6mR5FOHamPBG064zB+AFeWH94NdvaCBm8qnd" crossorigin="anonymous">

    <!-- Custom Fonts -->
    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="//oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="//oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- Gallery -->
    <link href="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.css" type="text/css" rel="stylesheet" />

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/style.css">

    <!-- Google Analytics -->
    


</head>


<body>

<div class="bg-gradient"></div>
<div class="bg-pattern"></div>

<!-- Menu -->
<!--Menu Links and Overlay-->
<div class="menu-bg">
    <div class="menu-container">
        <ul>
            
            <li class="menu-item">
                <a href="/">
                    Home
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/archives">
                    Archives
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/about">
                    About
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/tags">
                    Tags
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/categories">
                    Categories
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/contact.html">
                    Contact
                </a>
            </li>
            
        </ul>
    </div>
</div>

<!--Hamburger Icon-->
<nav>
    <a href="#menu"></a>
</nav>

<div class="container">

    <!-- Main Content -->
    <div class="row">
    <div class="col-sm-12">

        <!--Title and Logo-->
        <header>
    <div class="logo">
        <a href="/"><i class="logo-icon fa fa-cube" aria-hidden="true"></i></a>
        
    </div>
</header>

        <section class="main">
            
<div class="post">

    <div class="post-header">
        <h1 class="title">
            <a href="/2020/01/14/fnlp-%EB%B0%91%EB%B0%94%EB%8B%A5%EB%B6%80%ED%84%B0-%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94-%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC-1/">
                [FNLP] 밑바닥부터 시작하는 자연어처리 (1)
            </a>
        </h1>
        <div class="post-info">
            
                <span class="date">2020-01-14</span>
            
            
            
                <span class="category">
                    <a href="/categories/natural-language-processing/">Natural Language Processing</a>
                </span>
            
        </div>
    </div>

    <div class="content">

        <!-- Gallery -->
        

        <!-- Post Content -->
        <h1 id="한국어-임베딩-서론"><a href="#한국어-임베딩-서론" class="headerlink" title="한국어 임베딩: 서론"></a>한국어 임베딩: 서론</h1><h6 id="본-포스팅은-“한국어-임베딩-이기창-”을-바탕으로-“밑바닥부터-시작하는-딥러-2-사이토-고키-”를-추가하여-정리한-내용입니다"><a href="#본-포스팅은-“한국어-임베딩-이기창-”을-바탕으로-“밑바닥부터-시작하는-딥러-2-사이토-고키-”를-추가하여-정리한-내용입니다" class="headerlink" title="본 포스팅은 “한국어 임베딩(이기창)”을 바탕으로 “밑바닥부터 시작하는 딥러 2(사이토 고키)”를 추가하여 정리한 내용입니다."></a>본 포스팅은 “한국어 임베딩(이기창)”을 바탕으로 “밑바닥부터 시작하는 딥러 2(사이토 고키)”를 추가하여 정리한 내용입니다.</h6><p><br></br></p>
<h4 id="한국어-임베딩"><a href="#한국어-임베딩" class="headerlink" title="한국어 임베딩"></a>한국어 임베딩</h4><p></p>

<h5 id="01-서론"><a href="#01-서론" class="headerlink" title="01 서론"></a>01 서론</h5><p></p>

<p>1.1 임베딩이란<br>1.2 임베딩의 역할</p>
<ul>
<li>단어/문장 간 관련도 계산</li>
<li>의미/문법 정보 함축</li>
<li>전이 학습<br>1.3 임베딩 기법의 역사와 종류</li>
<li>통계 기반에서 뉴럴 네트워크 기반으로</li>
<li>단어 수준에서 문장 수준으로</li>
<li>룰, 엔드투엔드, 프리트레인/파인 튜닝</li>
<li>임베딩의 종류와 성능</li>
</ul>
<p><br></br></p>
<h4 id="밑바닥부터-시작하는-딥러닝-2"><a href="#밑바닥부터-시작하는-딥러닝-2" class="headerlink" title="밑바닥부터 시작하는 딥러닝 2"></a>밑바닥부터 시작하는 딥러닝 2</h4><p></p>

<h5 id="CHAPTER-2-자연어와-단어의-분산-표현"><a href="#CHAPTER-2-자연어와-단어의-분산-표현" class="headerlink" title="CHAPTER 2 자연어와 단어의 분산 표현"></a>CHAPTER 2 자연어와 단어의 분산 표현</h5><p>2.1 자연어 처리란<br>2.2 시소러스</p>
<ul>
<li>WordNet</li>
<li>시소러스의 문제점<br>2.3 통계 기반 기법</li>
<li>파이썬으로 말뭉치 전처리하기</li>
<li>단어의 분산 표현</li>
<li>분포 가설</li>
<li>동시발생 행렬</li>
<li>벡터 간 유사도</li>
<li>유사 단어의 랭킹 표시</li>
</ul>
<p>2.4 통계 기반 기법 개선하기</p>
<ul>
<li>상호정보량</li>
<li>차원 감소</li>
<li>SVD에 의한 차원 감소</li>
<li>PTB 데이터셋</li>
<li>PTB 데이터셋 평가</li>
</ul>
<p><br></br></p>
<p><hr><br><br></br></p>
<h4 id="임베딩이란"><a href="#임베딩이란" class="headerlink" title="임베딩이란"></a>임베딩이란</h4><p></p>

<p>기계 번역, 요약, 문장 자동 생성이 화제다. 컴퓨터가 사람 말을 알아듣고 사람처럼 을 쓸 수 있다는 사실에 각계에서 주목하고 있다. 대규모 말뭉치로 잘 학습된 딥러닝 모델은 긴 문서를 잘 요약하고 그럴듯한 문장을 만들어낸다.</p>
<p></p>

<p>하지만 컴퓨터는 어디까지나 빠르고 효율적인 ‘계산기’일 뿐이다. 한마디로 컴퓨터는 인간이 사용하는 자연어를 있는 그대로 이해하는 것이 아니라 숫자(로 변형된 말이나 글)을 계산한다는 이야기다. 기계의 자연어 이해와 생성은 연산(computation)이나 처(processing)의 영역이다.</p>
<p></p>

<p>그렇다면 제기할 수 있는 질문이 몇 가지 있다. 표현력이 무한한 언어를 컴퓨터가 연할 수 있는 숫자로 바꿀 수는 있는 걸까? 만약 그럴 수 있다면 말과 글을 숫자로 변할 때 어떤 정보를 함축시킬 것인가? 정보 압축 과정에서 손실이 발생하지는 않을까? 그 손실은 어떻게 줄일 수 있을까?</p>
<p></p>

<p>자연어 처리 분야에서 임베딩(embedding)이란, 사람이 쓰는 자연어를 기계가 이해할  있는 숫자의 나열인 벡터로 바꾼 결과 혹은 그 일련의 과정 전체를 의미한다. 단어 문장 각각을 벡터로 변환해 벡터 공간으로 ‘끼워 넣는다(embed)’는 의미에서 임베딩이라는 이름이 붙었다.</p>
<p></p>

<p>임베딩이라는 개념은 자연어 처리 분야에서 꽤 오래전부터 사용한 것으로 보인다. 하만 본격적으로 통용되기 시작한 것은 딥러닝의 대부 요슈아 벤지오 연구 팀이 A Neural Probablistic Language Model(2003)을 발표하고 나서부터다.</p>
<p></p>

<p>우리가 상상할 수 있는 가장 간단한 형태의 임베딩은 단어의 빈도를 그대로 벡터로 사용하는 것이다. 다음은 소설가 현진건이 1924년 발표한 &lt;운수 좋은 날&gt;, 주요섭의 1935년 작품 &lt;사랑 손님과 어머니&gt;, 이효석의 1936년 작품 &lt;메밀꽃 필 무렵&gt;, 황석영의 1973년 작품 &lt;삼포 가는 길&gt; 등 단편소설들의 단어별 빈도표다.</p>
<p><br></br></p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"><center>구분</center></th>
<th style="text-align:center"><center>메밀꽃 필 무렵</center></th>
<th style="text-align:center"><center>운수 좋은 날</center></th>
<th style="text-align:center"><center>사랑 손님과 어머니</center></th>
<th style="text-align:center"><center>삼포 가는 길</center></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><strong>기차</strong></td>
<td style="text-align:center">0</td>
<td style="text-align:center">2</td>
<td style="text-align:center">10</td>
<td style="text-align:center">7</td>
</tr>
<tr>
<td style="text-align:center"><strong>막걸리</strong></td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center"><strong>선술집</strong></td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
</tr>
</tbody>
</table>
</div>
<p><br></br></p>
<p>위 표와 같은 빈도표를 단어-문서 행렬(Term-Document Matrix)이라고 부른다. 행은 단어, 열은 문서(여기서는 작품)에 대응한다. 참고로 위의 표를 만들 때 형태소 분석기 오픈소스인 은전한닢(Mecab)을 사용해 명사를 추출해 계산했다. 단어-문서 행렬을 축하는 방법은 4장에서 설명할 예정이다.</p>
<p></p>

<p>표에서 보면 운수 좋은 날이라는 문서의 임베딩은 [2, 1, 1]이다. 막걸리라는 단어의 임베딩은 [0, 1, 0, 0]이다. 단어-문서 행렬을 보면 사랑 손님과 어머니, 삼포 가는 이 사용하는 단어 목록이 상대적으로 많이 겹치고 있는 것을 알 수 있다. 이를 바탕로 우리는 사랑 손님과 어머니는 삼포 가는 길과 기차라는 소재를 공유한다는 점에서 비슷한 작품일 것이라는 추정을 해볼 수 있다. 또 막걸리라는 단어와 선술집이라는 어가 운수 좋은 날이라는 작품에만 등장한 것을 알 수 있다. 이를 바탕으로 막걸리-술집 간 의미 차이가 막걸리-기차보다 작을 것이라고 추정해볼 수 있다.</p>
<p><br></br><br></p>
<h4 id="단어의-의미"><a href="#단어의-의미" class="headerlink" title="단어의 의미"></a>단어의 의미</h4><p>우리의 말은 ‘문자’로 구성되며, 말의 의미는 ‘단어’로 구성된다. 단어는 말하자면 의미의 최소 단위인 셈이다. 그래서 자연어를 컴퓨터에게 이해시키는 데는 무엇보다 ‘단어의 의미’를 이해시키는 게 중요하다. 이번 장의 주제는 컴퓨터에게 ‘단어의 의미’ 해시키기이다. 더 정확히 말하면 ‘단어의 의미’를 잘 파악하는 표현 방법에 관해 생해본다. 구체적으로 이번 장과 다음 장에서 다음의 세 가지 기법을 살펴보겠다.</p>
<p></p>

<ul>
<li>시소러스를 활용한 기법</li>
<li>통계 기반 기법</li>
<li>추론 기반 기법(Word2Vec)</li>
</ul>
<p></p>

<p>가장 먼저, 사람의 손으로 만든 시소러스(유의어 사전)을 이용하는 방법을 간단히 살펴보자. 그런 다음 통계 정보로부터 단어를 표현하는 ‘통계 기반 기법’을 설명한다. 여기까지가 이번 장에서 배우는 내용으로, 그 뒤를 이어 다음 장에서는 신경망을 활요한 ‘추론 기반’ 기법(구체적으로는 Word2Vec)을 다룬다.</p>
<p><br><br></p>
<h4 id="시소러스"><a href="#시소러스" class="headerlink" title="시소러스"></a>시소러스</h4><p>‘단어의 의미’를 나타내는 방법으로는 먼저 사람이 직접 단어의 의미를 정의하는 방식을 생각할 수 있다. 그중 한 방법으로 표준국어대사전처럼 각각의 단어에 그 의미를 설명해 넣을 수 있을 것이다. 예컨대 표준국어대사전에서 자동차라는 단어를 찾으면 원동기를 장치하여 그 동력으로 바퀴를 굴려서 철길이나 가설된 선에 의하지 아니하고 땅 위를 움직이도록 맍든 차라는 설명이 나온다. 이런 식으로 단어들을 정의해두면 컴퓨터도 단어의 의미를 이해할 수 있을지 모른다.</p>
<p></p>

<p>자연어 처리의 역사를 되돌아보면 단어의 의미를 인력을 동원해 정의하려는 시도는 수없이 있어왔다. 단, 표준국어대사전 같이 사람이 이용하는 일반적인 사전이 아니라 시소러스 형태의 사전을 애용했다. 시소러스란 (기본적으로는) 유의어 사전으로, ‘뜻이 같은 단어(동의어)’나 ‘뜻이 비슷한 단어(유의어)’가 한 그룹으로 분류되어 있다.</p>
<p></p>

<p>또한 자연어 처리에 이용되는 시소러스에서는 단어 사이의 ‘상위와 하위’ 혹은 ‘전체와 부분’ 등, 더 세세한 관계까지 정의해둔 경우가 있다. 아래의 예처럼 각 단어의 관계를 그래프 구조로 정의한다.</p>
<p></p>

<p><img src="/image/fnlp1.jpeg" alt="fnlp1"></p>
<p></p>

<p>위 그림에서는 car의 상위 개념으로 motor vehicle이라는 단어가 존재한다. 한편 car의 하위 개념으로는 SUV, compact, hatch-back 등 더 구체적인 차종이 있음을 알려준다. 이처럼 모든 단어에 대한 유의어 집합을 만든 다음, 단어들의 관계를 그래프로 표현하여 단어 사이의 연결을 정의할 수 있다. 그러면 이 ‘단어 네트워크’를 이용하여 컴퓨터에게 단어 사이의 관계를 가르칠 수 있다. 이 정도면 컴퓨터에게 단어의 의미를 (간접적으로라도) 이해시켰다고 주장할 수 있을 것이다. 그리고 그 지식을 이용하면 우리에게 유용한 일들을 컴퓨터가 수행하도록 할 수 있을 것이다.</p>
<p></p>

<p>시소러스를 어떻게 사용하는가는 자연어 처리 애플리케이션에 따라 다르다. 검색 엔진을 예로 생각해보면 automobile과 car가 유의어임을 알고 있으면 car의 검색 결과에 automobile의 검색 결과도 포함시켜주면 좋을 것이다.</p>
<p><br><br></p>
<h4 id="WordNet"><a href="#WordNet" class="headerlink" title="WordNet"></a>WordNet</h4><p>자연어 처리 분야에서 가장 유명한 시소러스는 WordNet이다. 이를 사용하면 유의어를 얻거나 ‘단어 네트워크’를 이용할 수 있다. 또한 단어 네트워크를 사용해 단어 사이의 유사도를 구할 수도 있다. 이 포스팅에서는 WordNet을 자세히 설명하지는 않겠다.</p>
<p><br></br></p>
<h4 id="시소러스의-문제점"><a href="#시소러스의-문제점" class="headerlink" title="시소러스의 문제점"></a>시소러스의 문제점</h4><p>WordNet과 같은 시소러스에는 수많은 단어에 대한 동의어와 계층 구조 등의 관계가 정의돼있다. 그리고 이 지식을 이용하면 ‘단어의 의미’를 (간접적으로라도) 컴퓨터에 전달할 수 있다. 하지만 이처럼 사람이 수작업으로 레이블링하는 방식에는 크나큰 결점이 존재한다. 다음은 시소러스 방식의 대표적인 문제점들이다.</p>
<ul>
<li>시대 변화에 대응하기 어렵다.</li>
<li>사람을 쓰는 비용은 크다.</li>
<li>단어의 미묘한 차이를 표현할 수 없다.</li>
</ul>
<p>이처럼 시소러스를 사용하는 기법(단어의 의미를 사람이 정의하는 기법)에는 커다란 문제가 있다. 이 문제를 피하기 위해, 곧이어 ‘통계 기반 기법’과 신경망을 사용한 ‘추론 기반 기법’을 알아볼 것이다. 이 두 기법에서는 대량의 텍스트 데이터로부터 ‘단어의 의미’를 자동적으로 추출한다. 그 덕분에 사람은 손수 단어를 연결짓는 중노동에서 해방되는 것이다.</p>
<p><br><br></p>
<h4 id="통계-기반-기법"><a href="#통계-기반-기법" class="headerlink" title="통계 기반 기법"></a>통계 기반 기법</h4><p>이제부터 통계 기반 기법을 살펴보면서 우리는 말뭉치(corpus)를 이용할 것이다. 말뭉치란 간단히 말하면 대량의 텍스트 데이터이다. 다만 맹목적으로 수집된 텍스트 데이터가 아닌 자연어 처리 연구가 애플리케이션을 염두에 두고 수집된 텍스트 데이터를 일반적으로 말뭉치라고 한다.</p>
<p></p>

<p>결국 말뭉치란 텍스트 데이터에 지나지 않다. 그 안에 담긴 문장들은 사람이 쓴 글이다. 다른 시각에서 생각해보면, 말뭉치에는 자연어에 대한 사람의 ‘지식’이 충분히 담겨 있다고 볼 수 있다. 문장을 쓰는 방법, 단어를 선택하는 방법, 단어의 의미 등 사람이 알고 있는 자연어에 대한 지식이 포함되어 있는 것이다. 통계 기반 기법의 목표는 이처럼 사람의 지식으로 가득한 말뭉치에사 자동으로, 그리고 효율적으로 그 핵심을 추출하는 것이다.</p>
<p></p>

<p>자연어 처리에 사용되는 말뭉치에는 텍스트 데이터에 대한 추가 정보가 포함되는 경우가 있다. 예컨대 텍스트 데이터의 단어 각각에 ‘품사’가 레이블링 될 수 있다. 이럴 경우 말뭉치는 컴퓨터가 다루기 쉬운 형태(트리 구조 등)로 가공되어 주어지는 것이 일반적이다. 본 포스팅에서는 이러한 추가 레이블을 이용하지 않고, 단순한 텍스트 데이터(하나의 큰 텍스트 파일)로 주어졌다고 가정한다.</p>
<p><br><br></p>
<h4 id="파이썬으로-말뭉치-전처리하기"><a href="#파이썬으로-말뭉치-전처리하기" class="headerlink" title="파이썬으로 말뭉치 전처리하기"></a>파이썬으로 말뭉치 전처리하기</h4><p>자연어 처리에는 다양한 말뭉치가 사용된다. 유명한 것으로는 위키백과와 구글 뉴스 등의 텍스트 데이터를 들 수 있다. 또한 셰익스피어나 나쓰메 소세키 같은 대문호의 작품들도 말뭉치로 이용된다. 이번 장에서는 우선 문장 하나로 이뤄진 단순한 텍스트를 사용한다. 그런 후에 더 실용적인 말뭉치도 다뤄보겠다.</p>
<p></p>

<p>그러면 파이썬을 이용하여 매우 작은 텍스트 데이터(말뭉치)에 전처리를 해보자. 여기서 말하는 전처리란 텍스트 데이터를 단어로 분할하고 그 분할된 단어들을 단어 ID 목록으로 변환하는 일이다.</p>
<p></p>

<p>그럼 하나씩 확인하면서 단계별로 진행해보자. 이번 절에서는 이처럼 문장 하나로 이뤄진 텍스트를 말뭉치로 이용한다. 실전이라면 이 text에 수천, 수만 개가 넘는 문장이 (연이어) 담겨 있을 것이다. 하지만 지금은 쉽게 설명하기 위해 이 작은 텍스트 데이터만으로 전처리를 수행하겠다. 그럼 이 text를 단어 단위로 분할하자.</p>
<p></p>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">text = <span class="string">"You say goodbye and I say hello."</span></span><br><span class="line"></span><br><span class="line">text = text.lower()</span><br><span class="line">text = text.replace(<span class="string">'.'</span>, <span class="string">' .'</span>)</span><br><span class="line">print(text)</span><br><span class="line"></span><br><span class="line">words = text.split(<span class="string">' '</span>)</span><br><span class="line">print(words)</span><br></pre></td></tr></table></figure>
<p></p>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 실행결과</span></span><br><span class="line">you say goodbye <span class="keyword">and</span> i say hello .</span><br><span class="line">[<span class="string">'you'</span>, <span class="string">'say'</span>, <span class="string">'goodbye'</span>, <span class="string">'and'</span>, <span class="string">'i'</span>, <span class="string">'say'</span>, <span class="string">'hello'</span>, <span class="string">'.'</span>]</span><br></pre></td></tr></table></figure>
<p></p>

<p>가장 먼저 lower() 메서드를 사용해 모든 문자를 소문자로 변환한다. 문장 첫머리의 대문자로 시작하는 단어도 소문자 단어와 똑같이 취급하기 위한 조치이다. 그리고 split(‘ ‘)메서드를 호출해 공백을 기준으로 분할한다. 다만 여기에서는 문장 끝의 마침표(.)를 고려해 마침표 앞에 공백을 삽입한 다음 분할을 수행하였다.</p>
<p></p>

<p>여기에서 단어를 분할할 때 마침효 앞에 공백을 넣는 임시변통을 적용했지만, 더 현명하고 범용적인 방법이 있다. 바로 정규표현식을 이용하는 방법이다. 예를 들어 정규표현식 모듈인 re를 임포트하고 re.split(‘(\W+)?’.text)라고 호출하면 단어 단위로 분할할 수 있다.</p>
<p></p>

<p>이제 원래의 문장을 단어 목록 형태로 이용할 수 있게 되었다. 단어 단위로 분할되어 다루기가 쉬워진 것을 사실이지만, 단어를 텍스트 그대로 조작하기란 여러 면에서 불편하다. 그래서 단어에 ID를 부여하고, ID의 리스트로 이용할 수 있도록 한 번 더 손질한다. 이를 위한 사전 준비로, 파이썬의 딕셔너리를 이용하여 단어 ID와 단어를 짝지어주는 대응표를 작성한다.</p>
<p></p>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">word_to_id = &#123;&#125;</span><br><span class="line">id_to_word = &#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">    <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> word_to_id:</span><br><span class="line">        new_id = len(word_to_id)</span><br><span class="line">        word_to_id[word] = new_id</span><br><span class="line">        id_to_word[new_id] = word</span><br><span class="line"></span><br><span class="line">print(word_to_id)</span><br><span class="line">print(id_to_word)</span><br></pre></td></tr></table></figure>
<p></p>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 실행결과</span></span><br><span class="line"></span><br><span class="line">&#123;<span class="string">'you'</span>: <span class="number">0</span>, <span class="string">'say'</span>: <span class="number">1</span>, <span class="string">'goodbye'</span>: <span class="number">2</span>, <span class="string">'and'</span>: <span class="number">3</span>, <span class="string">'i'</span>: <span class="number">4</span>, <span class="string">'hello'</span>: <span class="number">5</span>, <span class="string">'.'</span>: <span class="number">6</span>&#125;</span><br><span class="line">&#123;<span class="number">0</span>: <span class="string">'you'</span>, <span class="number">1</span>: <span class="string">'say'</span>, <span class="number">2</span>: <span class="string">'goodbye'</span>, <span class="number">3</span>: <span class="string">'and'</span>, <span class="number">4</span>: <span class="string">'i'</span>, <span class="number">5</span>: <span class="string">'hello'</span>, <span class="number">6</span>: <span class="string">'.'</span>&#125;</span><br></pre></td></tr></table></figure>
<p></p>

<p>단어 ID에서 단어로의 변환은 id_to_word가 담당하며(키가 단어 ID, 값이 단어), 단어에서 단어 ID로의 변환은 word_to_id가 담당한다. 앞의 코드는 단어 단위로 분할된 words의 각 원소를 처음부터 하나씩 살펴보면서, 단어가 word_to_id에 들어 있지 않으면 word_to_id와 id_to_word 각각에 새로운 ID와 단어를 추가한다. 또한 추가 시점의 딕셔너리 길이가 새로운 단어의 ID로 설정되기 때문에 단어 ID는 0, 1, 2, … 식으로 증가한다. 이처럼 딕셔너리를 사용하면 단어를 가지고 단어 ID를 검색하거나, 반대로 단어 ID를 가지고 단어를 검색할 수 있다.</p>
<p></p>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">print(id_to_word[<span class="number">5</span>])</span><br><span class="line">print(word_to_id[<span class="string">'hello'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 실행결과</span></span><br><span class="line">hello</span><br><span class="line"><span class="number">5</span></span><br></pre></td></tr></table></figure>
<p></p>

<p>그럼 마지막으로 ‘단어 목록’을 ‘단어 ID 목록’으로 변경해보자. 다음 코드에서는 파이썬의 comprehension 표기를 사용하여 단어 목록에서 단어 ID 목록으로 변환한 다음, 다시 넘파이 배열로 변환하였다.</p>
<p></p>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">ids = []</span><br><span class="line"><span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">    ids.append(word_to_id[word])</span><br><span class="line">print(ids)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">corpus = [word_to_id[word] <span class="keyword">for</span> word <span class="keyword">in</span> words]</span><br><span class="line">print(corpus)</span><br><span class="line">corpus = np.array(corpus)</span><br><span class="line">print(corpus)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 실행결과</span></span><br><span class="line">[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">1</span>, <span class="number">5</span>, <span class="number">6</span>]</span><br><span class="line">[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">1</span>, <span class="number">5</span>, <span class="number">6</span>]</span><br><span class="line">[<span class="number">0</span> <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> <span class="number">1</span> <span class="number">5</span> <span class="number">6</span>]</span><br></pre></td></tr></table></figure>
<p></p>

<p>이것으로 말뭉치를 이용하기 위한 사전 준비를 마쳤다. 이상의 처리를 한 데 모아 preprocess()라는 함수로 구현해보자.</p>
<p></p>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess</span><span class="params">(text)</span>:</span></span><br><span class="line">    text = text.lower()</span><br><span class="line">    text = text.replace(<span class="string">'.'</span>, <span class="string">' .'</span>)</span><br><span class="line">    words = text.split(<span class="string">' '</span>)</span><br><span class="line">    word_to_id = &#123;&#125;</span><br><span class="line">    id_to_word = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> word_to_id:</span><br><span class="line">            new_id = len(word_to_id)</span><br><span class="line">            word_to_id[word] = new_id</span><br><span class="line">            id_to_word[new_id] = word</span><br><span class="line">    corpus = np.array(word_to_id[word] <span class="keyword">for</span> word <span class="keyword">in</span> words)</span><br><span class="line">    <span class="keyword">return</span> corpus, word_to_id, id_to_word</span><br><span class="line"></span><br><span class="line">test = <span class="string">'I love you so much.'</span></span><br><span class="line">corpus, word_to_id, id_to_word = preprocess(test)</span><br><span class="line">print(corpus, word_to_id, id_to_word)</span><br></pre></td></tr></table></figure>
<p></p>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 실행결과</span></span><br><span class="line">&lt;generator object preprocess.&lt;locals&gt;.&lt;genexpr&gt; at <span class="number">0x7fde7022a468</span>&gt;</span><br><span class="line">&#123;<span class="string">'i'</span>: <span class="number">0</span>, <span class="string">'love'</span>: <span class="number">1</span>, <span class="string">'you'</span>: <span class="number">2</span>, <span class="string">'so'</span>: <span class="number">3</span>, <span class="string">'much'</span>: <span class="number">4</span>, <span class="string">'.'</span>: <span class="number">5</span>&#125;</span><br><span class="line">&#123;<span class="number">0</span>: <span class="string">'i'</span>, <span class="number">1</span>: <span class="string">'love'</span>, <span class="number">2</span>: <span class="string">'you'</span>, <span class="number">3</span>: <span class="string">'so'</span>, <span class="number">4</span>: <span class="string">'much'</span>, <span class="number">5</span>: <span class="string">'.'</span>&#125;</span><br></pre></td></tr></table></figure>
<p></p>

<p>이것으로 말뭉치 전처리가 끝났다. corpus는 단어 ID 목록, word_to_id는 단어에서 단어 ID로의 딕셔너리, id_to_word는 단어 ID에서 단어로 딕셔너리를 뜻한다. 이상으로 말뭉치를 다룰 준비를 마쳤다. 다음 목표는 말뭉치를 사용해 ‘단어의 의미’를 추출하는 것이다. 그 한 방법으로, 이번 절에서는 ‘통계 기반 기법’을 살펴보자. 이 기법을 사용해 우리는 단어를 벡터로 표현할 수 있게 될 것이다.</p>
<p><br></br><br></p>
<h4 id="단어의-분산-표현"><a href="#단어의-분산-표현" class="headerlink" title="단어의 분산 표현"></a>단어의 분산 표현</h4><p>뜬금없게 드릴겠지만, 세상은 다채로운 ‘색’으로 가득하다. 이러한 색들에는 ‘코발트블루’나 ‘싱크레드’ 같은 고유한 이름을 붙일 수도 있다. 한편, RGB라는 세 가지 성분이 어떤 비율로 섞여 있느냐로 표현하는 방법도 있다. 전자는 색의 가짓수만큼의 이름을 부여하는 한편, 후자는 색을 3차원의 벡터로 표현한다.</p>
<p></p>

<p>여기서 주목하고 싶은 점은 RGB 같은 벡터 표현이 색을 더 정확하게 명시할 수 있다는 사실이다. 게다가 모든 색을 단 3개의 성분으로 간결하게 표현할 수 있고, (많은 경우) 어떤 색인지 짐작하기도 쉽다. 예컨대 비색이라고 하면 어떤 색인지 몰라도, (R, G, B) = (170, 33, 22)라고 하면 빨강 계열의 색임을 알 수 있다. 또한 색끼리의 관련성(비슷한 색인지 여부 등)도 벡터 표현 쪽이 더 쉽게 판단할 수 있고, 정량화하기도 쉽다.</p>
<p></p>

<p>그러면 색을 벡터로 표현하듯 단어도 벡터로 표현할 수 있을까? (조금 어렵긴 하지만) 더 정확하게 말하자면, 간결하고 이치에 맞는 벡터 표현을 단어라는 영역에서도 구축할 수 있을까? 이제부터 우리가 원하는 것은 단어의 의미를 정확하게 파악할 수 있는 벡터 표현이다. 이를 자연어 처리 분야에서는 단어의 분산 표현(distributed representation)이라고 한다.</p>
<p></p>

<p>단어의 분산 표현은 단어를 고정 길이의 밀집벡터(dense vector)로 표현한다. 밀집벡터라 함은 대부분의 원소가 0이 아닌 실수인 벡터를 말한다. 예컨대 3차원의 분산 표현은 [0.21, -0.45, 0.83]과 같은 모습이 된다. 이러한 단어의 분산 표현을 어떻게 구축할 것인가가 앞으로 살펴볼 중요한 주제이다.</p>
<p><br></br><br></p>
<h4 id="분포-가설"><a href="#분포-가설" class="headerlink" title="분포 가설"></a>분포 가설</h4><p>자연어 처리의 역사에서 단어를 벡터로 표현하는 연구는 수없이 이뤄져 왔다. 그 연구들을 살펴보면, 중요한 기법의 거의 모두가 단 하나의 간단한 아이디어에 뿌리를 두고 있음을 알 수 있다. 그 아이디어는 바로 ‘단어의 의미는 주변 단어에 의해 형성된다’라는 것이다. 이를 분포 가설(distributional hypothesis)이라 하며, 단어를 벡터로 표현하는 최근 연구도 대부분 이 가설에 기초한다.</p>
<p></p>

<p>분포 가설이 말하고자 하는 바는 매우 간단하다. 단어 자체에는 의미가 없고, 그 단어가 사용된 맥락(context)이 의미를 형성한다는 것이다. 물론 의미가 같은 단어들은 같은 맥락에서 더 많이 등장한다. 예컨대 I drink beer와 We drink wine 처럼 drink 주변에는 음료가 등장하기 쉬울 것이다. 또 I guzzle beer와 We guzzle wine이라는 문장이 있다면, guzzle은 drink와 같은 맥락에서 사용됨을 알 수 있다. 그리고 guzzle과 drink는 가까운 의미의 단어라는 것도 알 수 있다.</p>
<p></p>

<p>앞으로는 맥락이라는 말을 자주 사용할 것이다. 이번 장에서 맥락이라 하면 (주목하는 단어) 주변에 놓인 단어를 가리킨다. 예컨대 아래 그림에서는 좌우의 각 두 단어씩이 맥락에 해당한다.</p>
<p></p>

<p><img src="/image/fnlp2.jpeg" alt="fnlp2"></p>
<p></p>

<p>위 그림처럼 ‘맥락’이란 특정 단어를 중심에 둔 그 주변 단어를 말한다. 그리고 맥락의 크기(주변 단어를 몇 개나 포함할지)를 ‘윈도우 크기(window size)’라고 한다. 윈도우 크기가 1이면 좌주 한 단어씩이, 윈도우 크기가 2이면 좌우 두 단어씩이 맥락에 포함된다.</p>
<p></p>

<p>여기에서는 좌우로 똑같은 수의 단어를 맥락으로 사용했다. 하지만 상황에 따라서는 왼쪽 단어만 또는 오른쪽 단어만을 사용하기도 하며, 문장의 시작과 끝을 고려할 수도 있다. 본 포스팅에서는 문장 구분은 고려하지 않고 좌우 동수인 맥락만을 취급한다.</p>
<p><br></br><br></p>
<h4 id="동시발생-행렬"><a href="#동시발생-행렬" class="headerlink" title="동시발생 행렬"></a>동시발생 행렬</h4><p>그러면 분포 가설에 기초해 단어를 벡터로 나타내는 방법을 생각해봅시다. 주변 단어를 ‘세어보는’ 방법이 자연스럽게 떠오를 것입니다. 무슨 말인고 하니, 어떤 단어에 주목했을 때, 그 주변에 어떤 단어가 몇 번이나 등장하는지를 세어 집계하는 방법이다. 이 책에서는 이를 통계기반 기법이라고 한다. 먼저 위에서 살펴보았던 파이썬으로 말뭉치 전처리하기 절에서 봤던 preprocess() 함수를 사용해 전처리하는 일부터 시작하자.</p>
<p></p>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">'..'</span>)</span><br><span class="line"><span class="keyword">from</span> util <span class="keyword">import</span> preprocess</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">text = <span class="string">'You say goodbye and I say hello.'</span></span><br><span class="line">corpus, word_to_id, id_to_word = preprocess(text)</span><br><span class="line"></span><br><span class="line">print(corpus)</span><br><span class="line"><span class="comment"># [0 1 2 3 4 1 5 6]</span></span><br><span class="line"></span><br><span class="line">print(id_to_word)</span><br><span class="line"><span class="comment"># &#123;0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'&#125;</span></span><br></pre></td></tr></table></figure>
<p></p>

<p>결과를 보면 단어 수가 총 7개임을 알 수 있다. 다음으로는 각 단어의 맥락에 해당하는 단어의 빈도를 세어보겠다. 윈도우 크기는 1로 하고, 단어 ID가 0인 you부터 시작해보자. you의 맥락은 say라는 단어 하나뿐이다. 단어 you의 맥락으로써 동시에 발생(등장)하는 단어의 빈도를 바탕으로 you라는 단어를 벡터로 표현하면 [0, 1, 0, 0, 0, 0, 0]와 같다. 계속해서 ID가 1인 say에 대해서도 같은 작업을 수행한다. 그 결과로부터 say라는 단어는 벡터 [1, 0, 1, 0, 1, 1, 0]으로 표현할 수 있다. 이상의 작업을 모든 단어(이번 예에서는 총 7개의 단어)에 대해서 수행한 결과가 아래와 같다.</p>
<p></p>

<p><img src="/image/fnlp3.jpeg" alt="fnlp3"></p>
<p></p>

<p>위 그림은 모든 단어에 대해 동시발생하는 단어를 표에 정리한 것이다. 이 표의 각 행은 해당 단어를 표현한 벡터가 된다. 참고로 이 표가 행렬의 형태를 띤다는 뜻에서 동시발생 행렬(co-occurrence matrix)라고 한다. 그럼 파이썬에서 그림 그대로를 손으로 입력해보겠다.</p>
<p></p>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">C = np.array([</span><br><span class="line">    [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">], dtype=np.int32)</span><br><span class="line"></span><br><span class="line">print(C[<span class="number">0</span>])  <span class="comment"># ID가 0인 단어의 벡터 표현</span></span><br><span class="line">print(C[<span class="number">4</span>])  <span class="comment"># ID가 4인 단어의 벡터 표현</span></span><br><span class="line">print(C[word_to_id[<span class="string">'goodbye'</span>]])  <span class="comment"># 'goodbye'의 벡터 표현</span></span><br></pre></td></tr></table></figure>
<p></p>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 실행결과</span></span><br><span class="line">[<span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">[<span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">[<span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<p></p>

<p>이와 같이 동시발생 행렬을 활용하면 단어를 벡터로 나타낼 수 있다. 이번에는 동시발생 행렬을 수동으로 만들었지만, 당연히 자동화할 수 있다. 그러면 말뭉치로부터 동시발생 행렬을 만들어주는 함수를 구현해보자. 함수 이름은 create_co_matrix(corpus, vocab_size, window_size=1)로 하자. 인수들은 차례로 단어 ID의 리스트, 어휘 수, 윈도우 크기를 나타낸다.</p>
<p></p>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_co_matrix</span><span class="params">(corpus, vocab_size, window_size=<span class="number">1</span>)</span>:</span></span><br><span class="line">    corpus_size = len(corpus)  <span class="comment"># 문장길이</span></span><br><span class="line">    co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> idx, word_id <span class="keyword">in</span> enumerate(corpus):</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, window_size + <span class="number">1</span>):</span><br><span class="line">            left_idx = idx - <span class="number">1</span></span><br><span class="line">            right_idx = idx + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> left_idx &gt;= <span class="number">0</span>:</span><br><span class="line">                co_matrix[word_id][corpus[left_idx]] += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> right_idx &lt; corpus_size:</span><br><span class="line">                co_matrix[word_id][corpus[right_idx]] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> co_matrix</span><br></pre></td></tr></table></figure>
<p><br></br><br></p>
<h4 id="벡터-간-유사도"><a href="#벡터-간-유사도" class="headerlink" title="벡터 간 유사도"></a>벡터 간 유사도</h4><p>앞에서 동시발생 행렬을 활용해 단어를 벡터로 표현하는 방법을 알아봤다. 그럼 계속해서 벡터 사이의 유사도를 측정하는 방법을 살펴보자. 벡터 사이의 유사도를 측정하는 방법은 다양하다. 대표적으로는 벡터의 내적이나 유클리드 거리 등을 꼽을 수 있다. 그 외에도 다양하지만, 단어 벡터의 유사도를 나타낼 때는 코사인 유사도를 자주 이용한다. 두 벡터 $\vec{x}\mathit{=(x_1, x_2, …, x_n)}$과 $\vec{y}\mathit{=(y_1, y_2, …, y_n)}$이 있다면, 코사인 유사도는 다음 식으로 정의됩니다.</p>
<p></p>

<script type="math/tex; mode=display">similarity( \vec{x}, \vec{y}) = \frac{\vec{x} \cdot \vec{y}}{||\vec{x}|| ||\vec{y}||}</script><p></p>

<p>위 식의 분자에는 벡터의 내적이, 분모에는 각 벡터의 노름(norm)이 등장한다. 노름은 벡터의 크기를 나타낸 것으로, 여기에서는 벡터의 각 원소를 제곱해 더한 후 다시 제곱근을 구해 계산하는 L2 노름을 구한다. 이 식의 핵심은 벡터를 정규화하고 내적을 구하는 것이다. 코사인 유사도를 직관적으로 풀어보자면 두 벡터가 가리키는 방향이 얼마나 비슷한가이다. 두 벡터의 방향이 완전히 같다면 코사인 유사도가 1이 되며, 완전히 반대라면 -1이 된다.</p>
<p></p>

<p>이제 코사인 유사도를 파이썬 함수로 구현해보자.</p>
<p></p>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cos_similarity</span><span class="params">(x, y)</span>:</span></span><br><span class="line">    nx = x / np.sqrt(np.sum(x**<span class="number">2</span>))  <span class="comment"># x의 정규화</span></span><br><span class="line">    ny = y / np.sqrt(np.sum(y**<span class="number">2</span>))  <span class="comment"># y의 정규화</span></span><br><span class="line">    <span class="keyword">return</span> np.dot(nx, ny)</span><br></pre></td></tr></table></figure>
<p></p>

<p>이 코드에서 인수 x와 y는 넘파이 배열이라고 가정하자. 이 함수는 먼저 벡터 x와 y를 정규화한 후 두 벡터의 내적을 구했다. 이렇게만 해도 코사인 유사도를 구할 수 있지만, 사실 이 구현에는 문제가 하나 있다. 인수로 제로 벡터(원소가 모두 0인 벡터)가 들어오면 0으로 나누기 오류가 발생한다. 이 문제를 해결하는 전통적인 방법은 나눌 때 분모에 작은 값을 더해주는 것이다. 작은 값을 뜻하는 eps를 인수로 받도록 하고, 이 인수의 값을 지정하지 않으면 기본값으로 1e-8(0.00000001)이 설정되도록 수정하겠다. 이제 개선된 코드를 보자.</p>
<p></p>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cos_similarity</span><span class="params">(x, y, eps=<span class="number">1e-8</span>)</span>:</span></span><br><span class="line">    nx = x / np.sqrt(np.sum(x**<span class="number">2</span>) + eps)  <span class="comment"># x의 정규화</span></span><br><span class="line">    ny = y / np.sqrt(np.sum(y**<span class="number">2</span>) + eps)  <span class="comment"># y의 정규화</span></span><br><span class="line">    <span class="keyword">return</span> np.dot(nx, ny)</span><br></pre></td></tr></table></figure>
<p></p>

<p>여기에서는 작은 값으로 1e-8을 사용했는데, 이 정도 작은 값이면 일반적으로 부동소수점 계산 시 반올림되어 다른 값에 흡수된다. 앞의 구현에서는 이 값이 벡터의 노름에 흡수되기 때문에 대부분의 경우 eps를 더한다고 해서 최종 계산 결과에는 영향을 주지 않는다. 한편 벡터의 노름이 0일 때는 이 작은 값이 그대로 유지되어 0으로 나누기 오류가 나는 사태를 막아준다.</p>
<p></p>

<p>이 함수를 사용하면 단어 벡터의 유사도를 다음과 같이 구할 수 있다. 다음은 you와 i의 유사도를 구하는 코드이다.</p>
<p></p>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">T = create_co_matrix(corpus, vocab_size)</span><br><span class="line"></span><br><span class="line">t0 = T[word_to_id[<span class="string">'you'</span>]]  <span class="comment"># you의 단어 벡터</span></span><br><span class="line">t1 = T[word_to_id[<span class="string">'i'</span>]]  <span class="comment"># i의 단어 벡터</span></span><br><span class="line"></span><br><span class="line">print(cos_similarity(t0, t1))</span><br><span class="line"><span class="comment"># 0.7071067758832467</span></span><br></pre></td></tr></table></figure>
<p></p>

<p>실행 결과 you와 i의 코사인 유사도는 0.70…으로 나왔다. 코사인 유사도 값은 -1에서 1 사이므로, 이 값은 비교적 높다(유사성이 크다)고 말할 수 있다.</p>
<p><br></br><br></p>
<h4 id="유사-단어의-랭킹-표시"><a href="#유사-단어의-랭킹-표시" class="headerlink" title="유사 단어의 랭킹 표시"></a>유사 단어의 랭킹 표시</h4><p>코사인 유사도까지 구현했으니 이 함수를 활용해 또 다른 유용한 기능을 구현해보고 싶다. 어떤 단어가 검색어로 주어지면, 그 검색어와 비슷한 단어를 유사도 순으로 출력하는 함수는 어떨까? 그 함수 이름은 most_similar()로 하고, 다음 인수들을 입력받도록 구현해보자.</p>
<p></p>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">most_similar(query, word_to_id, id_to_word, word_matrix, top=<span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<p></p>

<ul>
<li>query : 검색어(단어)</li>
<li>word_to_id : 단어에서 단어 ID로의 딕셔너리</li>
<li>id_to_word : 단어 ID에서 단어로의 딕셔너리</li>
<li>word_matrix : 단어 벡터들을 한데 모은 행렬. 각 행에는 대응하는 단어의 벡터가 저장되어 있다고 가정한다.</li>
<li>top : 상위 몇 개까지 출력할지 설정</li>
</ul>
<p></p>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">most_similar</span><span class="params">(query, word_to_id, id_to_word, word_matrix, top=<span class="number">5</span>)</span>:</span></span><br><span class="line">    <span class="comment"># (1) 검색어를 꺼낸다.</span></span><br><span class="line">    <span class="keyword">if</span> query <span class="keyword">not</span> <span class="keyword">in</span> word_to_id:</span><br><span class="line">        print(<span class="string">'%s(을)를 찾을 수 없습니다.'</span> % query)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">    print(<span class="string">'\n[query] '</span> + query)</span><br><span class="line">    query_id = word_to_id[query]</span><br><span class="line">    query_vec = word_matrix[query_id]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># (2) 코사인 유사도 계산</span></span><br><span class="line">    vocab_size = len(id_to_word)</span><br><span class="line">    similarity = np.zeros(vocab_size)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(vocab_size):</span><br><span class="line">        similarity[i] = cos_similarity(word_matrix[i], query_vec)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># (3) 코사인 유사도를 기준으로 내림차순으로 출력</span></span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> (<span class="number">-1</span> * similarity).argsort():</span><br><span class="line">        <span class="keyword">if</span> id_to_word[i] == query:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        print(<span class="string">'%s: %s'</span> %(id_to_word[i], similarity[i]))</span><br><span class="line"></span><br><span class="line">        count += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> count &gt;= top:</span><br><span class="line">            <span class="keyword">return</span></span><br></pre></td></tr></table></figure>
<p><br></br></p>
<p>이 코드는 다음 순서로 동작한다.</p>
<ol>
<li>검색어의 단어 벡터를 꺼낸다.</li>
<li>검색어의 단어 벡터와 다른 모든 단어 벡터와의 코사인 유사도를 각각 구한다.</li>
<li>계산한 코사인 유사도 결과를 기준으로 값이 높은 순서대로 출력한다.</li>
</ol>
<p><br></br></p>
<p>3번의 코드에 관해서만 설명을 덧붙이겠다. 여기서는 similarity 배열에 담긴 원소의 인덱스를 내림차순으로 정렬한 후 상위 원소들을 출력한다.이때 배열 인덱스의 정렬을 바꾸는데 사용한 argsort() 메서드는 너마이 배열의 원소를 오름차순으로 정렬한다((단, 반환값은 배열의 인덱스). 예를 하나 보자.</p>
<p></p>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([<span class="number">100</span>, <span class="number">-20</span>, <span class="number">2</span>])</span><br><span class="line">print(x.argsort())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 실행결과</span></span><br><span class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<p></p>

<p>이 짤막한 코드는 [100, -20, 2]라는 넘파이 배열의 원소들을 오름차순으로 정렬했다. 이때 반환된 배열에 담긴 원소들은 원래 배열의 인덱스에 해당한다. 즉, 앞의 결과는 인덱스가 1인 원소(-20), 2인 원소(2), 0인 원소(100) 순으로 정렬된 것이다. 여기서 우리의 목적은 단어의 유사도가 ‘큰’ 순서로 정렬하는 것이었다. 따라서 넘파이 배열의 각 원소에 마이너스를 곱한 후 argsort() 메서드를 호출하면 원하는 결과를 얻을 수 있다. 앞의 예에 적용해보면 다음과 같이 된다.</p>
<p></p>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">print((-x).argsort())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 실행결과</span></span><br><span class="line">[<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<p></p>

<p>이처럼 argsort()를 사용하면 단어의 유사도가 높은 순서로 출력할 수 있다. 이제 이 함수를 사용해볼 차례이다. you를 검색어로 지정해 유사한 단어들을 출력해보자.</p>
<p></p>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> util <span class="keyword">import</span> preprocess, create_co_matrix, most_similar</span><br><span class="line"></span><br><span class="line">x = np.array([<span class="number">100</span>, <span class="number">-20</span>, <span class="number">2</span>])</span><br><span class="line">print(x.argsort())</span><br><span class="line">print((-x).argsort())</span><br><span class="line"></span><br><span class="line">text = <span class="string">'You say goodbye and I say hello.'</span></span><br><span class="line">corpus, word_to_id, id_to_word = preprocess(text)</span><br><span class="line">vocab_size = len(word_to_id)</span><br><span class="line">C = create_co_matrix(corpus, vocab_size)</span><br><span class="line"></span><br><span class="line">most_similar(<span class="string">'you'</span>, word_to_id, id_to_word, C, top=<span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<p></p>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 실행결과</span></span><br><span class="line"></span><br><span class="line">[query] you</span><br><span class="line">goodbye: <span class="number">0.7071067758832467</span></span><br><span class="line">i: <span class="number">0.7071067758832467</span></span><br><span class="line">hello: <span class="number">0.7071067758832467</span></span><br><span class="line">say: <span class="number">0.0</span></span><br><span class="line"><span class="keyword">and</span>: <span class="number">0.0</span></span><br></pre></td></tr></table></figure>
<p></p>

<p>이 결과는 검색어 you와 유사한 단어를 상위 5개만 출력한 것이다. 코사인 유사도는 해당 단어의 오른쪽에서 볼 수 있다. 앞의 결과를 보면 you에 가장 가까운 단어는 총 3개인데, 차례로 goodbye, i, hello이다. 확실히 i와 you 모두 인칭대명사이므로 둘이 비슷하다는 건 납득이 된다. 하지만 goodbye와 hello의 코사인 유사도가 높다는 것은 우리의 직관과는 거리가 멀다. 물론 지금은 말뭉치의 크기가 너무 작다는 것이 원인이다. 나중에 더 큰 말뭉치를 사용하여 똑같은 실험을 해볼 테니 조금만 기다려 주자.</p>
<p></p>

<p>자, 지금까지 본 것처럼 동시발생 행렬을 이용하면 단어를 벡터로 표혀날 수 있다. 이것으로 통계 기반 기법의 기본을 끝마치겠다. 지금까지가 기본인 만큼, 아직 이야기할 주제는 얼마든지 남아 있다. 다음 절에서는 지금의 방법을 한층 개선하는 아이디어를 설명하고 실제로 구현해볼 것이다.</p>
<p><br></br><br></p>
<h4 id="통계-기반-기법-개선하기"><a href="#통계-기반-기법-개선하기" class="headerlink" title="통계 기반 기법 개선하기"></a>통계 기반 기법 개선하기</h4><p>앞 절에서는 단어의 동시발생 행렬을 만들었다. 이를 이용해 단어를 벡터로 표현하는 데는 성공했는데, 사실 동시발생 행렬에는 아직 개선할 점이 있다. 이번 절에서는 이 개선 작업을 해볼까 한다. 그리고 개선을 완료한 다음에는 좀 더 실용적인 말뭉치를 사용하여 ‘진짜’ 단어의 분산 표현을 손에 넣어보겠다.</p>
<p><br></br></p>
<h4 id="상호정보량"><a href="#상호정보량" class="headerlink" title="상호정보량"></a>상호정보량</h4><p>앞 절에서 본 동시발생 행렬의 원소는 두 단어가 동시에 발생한 횟수를 나타낸다. 그러나 이 ‘발생’ 횟수라는 것은 사실 그리 좋은 특징이 아니다. 고빈도 단어(많이 출현하는 단어)로 눈을 돌려보면 바로 그 이유를 알 수 있다.</p>
<p></p>

<p>예컨대 말뭉치에서 the와 car의 동시발생을 생각해보자. 분명 “…the car…”라는 문구가 자주 보일 것이다. 따라서 두 단어의 동시발생 횟수는 아주 많을 것이다. 한편 car와 drive는 확실히 관련이 깊다. 하지만 단순히 등장 횟수만 본다면 car는 drive보다는 the와의 관련성이 훨씬 강하다고 나올 것이다. the가 고빈도 단어라서 car와 강한 관련성을 갖는다고 평가되기 때문이다.</p>
<p></p>

<p>이 문제를 해결하기 위해 점별 상호정보량(Pointwise Mutual Information, PMI)이라는 척도를 사용한다. PMI는 확률 변수 $\mathit{x}, \mathit{y}$에 대해 다음 식으로 정의된다.</p>
<p></p>

<script type="math/tex; mode=display">PMI(x, y) = log_2 \frac{P(x, y)}{P(x) P(y)}</script><p></p>

<p>위 식에서 $\mathit{P(x)}$는 $\mathit{x}$가 일어날 확률, $\mathit{P(y)}$는 $\mathit{y}$가 일어날 확률, $\mathit{P(x, y)}$는 $\mathit{x}, \mathit{y}$가 동시에 일어날 확률을 뜻한다. 이 PMI 값이 높을수록 관련성이 높다는 의미이다.</p>
<p></p>

<p>이 식을 앞의 자연어 예에 적용하면 10,000개의 단어로 이뤄진 말뭉치에서 the가 100번 등장한다면 $\mathit{P(“the”)=0.01}$이 된다. 마찬가지로 the와 car가 10번 동시발생했다면 $\mathit{P(“the”, “car”)=0.001}$이 되는 것이다.</p>
<p></p>

<p>그럼 동시발생 행렬(각 원소는 동시발생한 단어의 횟수)을 사용하여 위의 식을 다시 써보자. $\mathit{C}$는 동시발생 행렬, $\mathit{C(x, y)}$는 단어 $\mathit{x, y}$가 동시발생하는 횟수, $\mathit{C(x), C(y)}$는 각 단어의 등장 횟수이다. 이때 말뭉치에 포함된 단어 수를 $\mathit{N}$이라 하면, 다음과 같이 변한다.</p>
<p></p>

<script type="math/tex; mode=display">PMI(x, y) = \log_{2} \frac{P(x)P(y)}{P(x, y)} = \log_{2} \frac{ \frac{C(x, y)}{N} }{ \frac{C(x)}{N} \frac{C(y)}{N} } = \log_{2} \frac{C(x, y)} \cdot N}{C(x) Cy)}</script><p></p>

<p>위 식에 따라 동시발생 행렬로부터 PMI를 구할 수 있다. 그러면 위 식대로 구체적인 계산을 해보자. 말뭉치의 단어 수를 10,000이라 하고, the와 car와 drive가 각각 1,000번, 20번, 10번 등장했다고 해보자. 그리고 the와 car의 동시발생 수는 10회, car와 drive의 동시발생 수는 5회라고 가정하자. 이 조건이라면, 동시발생 횟수 관점에서는 car는 drive보다 the와 관련이 깊다고 나온다. 그렇다면 PMI 관점에서는 어떨까?</p>
<p></p>

<script type="math/tex; mode=display">PMI("the", "car") \approx 2.32</script><script type="math/tex; mode=display">PMI("drive", "car") \approx 7.97</script><p></p>

<p>이 결과에서 알 수 있듯이 PMI를 이용하면 car와 the 보다 drive와의 관련성이 강해진다. 우리가 원하던 결과다. 이러한 결과가 나온 이유는 단어가 단독으로 출현하는 횟수가 고려되었기 때문이다. 이 예에서는 the가 자주 출현했으므로 PMI 점수가 낮아진 것이다. 이제 PMI라는 멋진 척도를 얻었지만, 이 PMI에도 한 가지 문제가 있다. 바로 두 단어의 동시발생 횟수가 0이면 음의 무한대가 된다는 점이다. 이 문제를 피하기 위해 실제로 구현할 때는 양의 상호정보량(Positive PMI)를 사용한다.</p>
<p></p>

<script type="math/tex; mode=display">PPMI(x, y) = max(0, PMI(x, y))</script><p></p>

<p>이 식에 따라 PMI가 음수일 때는 0으로 취급한다. 이제 단어 사이의 관련성을 0 이상의 실수로 나타낼 수 있다. 그러면 동시발생 행렬을 PPMI 행렬로 변환하는 함수를 구현해보자.</p>
<p></p>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ppmi</span><span class="params">(C, verbose=False, eps=<span class="number">1e-8</span>)</span>:</span></span><br><span class="line">    M = np.zeros_like(C, dtype=np.float32)</span><br><span class="line">    N = np.sum(C)</span><br><span class="line">    S = np.sum(C, axis=<span class="number">0</span>)</span><br><span class="line">    total = C.shape[<span class="number">0</span>] * C.shape[<span class="number">1</span>]</span><br><span class="line">    cnt = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(C.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(C.shape[<span class="number">1</span>]):</span><br><span class="line">            pmi = np.log2(C[i, j] * N / (S[j]*S[i]) + eps)</span><br><span class="line">            M[i, j] = max(<span class="number">0</span>, pmi)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> verbose:</span><br><span class="line">                cnt += <span class="number">1</span></span><br><span class="line">                <span class="keyword">if</span> cnt % (total//<span class="number">100</span>) == <span class="number">0</span>:</span><br><span class="line">                    print(<span class="string">'%.1f%% 완료'</span> % (<span class="number">100</span>*cnt/total))</span><br><span class="line">    <span class="keyword">return</span> M</span><br></pre></td></tr></table></figure>
<p></p>

<p>여기에서 인수 C는 동시발생 행렬, verbose는 진행상황 출력 여부를 결정하는 플래그이다. 큰 말뭉치를 다룰 때 verbose=True로 설정하면 중간중간 진행 상황을 알려준다. 참고로, 이 코드는 동시발생 행렬에 대해서만 PPMI 행렬을 구할 수 있도록 하고자 단순화해 구현했다. 구체적으로 말하면 $C(x) = \sum_i C(i, x), C(y) = \sum_i C(i, y), N = \sum_i \sum_j C(i, j)$가 되도록 (즉, 근사값을 구하도록) 구현했다. 한 가지 더, 이 코드에서는 np.log2(0)이 음의 무한대(-inf)가 되는 사태를 피하기 위해 eps라는 작은 값을 사용했다.</p>
<p></p>

<p>그럼, 동시발생 행렬을 PPMI 행렬로 변환해보자. 이는 다음처럼 구현할 수 있다.</p>
<p></p>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">'..'</span>)</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> util <span class="keyword">import</span> preprocess, create_co_matrix, cos_similarity, ppmi</span><br><span class="line"></span><br><span class="line">text = <span class="string">'You say goodbye and I say hello.'</span></span><br><span class="line">corpus, word_to_id, id_to_word = preprocess(text)</span><br><span class="line">vocab_size = len(word_to_id)</span><br><span class="line">C = create_co_matrix(corpus, vocab_size)</span><br><span class="line">W = ppmi(C)</span><br><span class="line"></span><br><span class="line">np.set_printoptions(precision=<span class="number">3</span>)  <span class="comment"># 유효 자릿수를 세 자리로 표시</span></span><br><span class="line">print(<span class="string">'동시발생 행렬'</span>)</span><br><span class="line">print(C)</span><br><span class="line">print(<span class="string">'-'</span>*<span class="number">50</span>)</span><br><span class="line">print(<span class="string">'PPMI'</span>)</span><br><span class="line">print(W)</span><br></pre></td></tr></table></figure>
<p></p>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 실행결과</span></span><br><span class="line"></span><br><span class="line">동시발생 행렬</span><br><span class="line">[[<span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]</span><br><span class="line">--------------------------------------------------</span><br><span class="line">PPMI</span><br><span class="line">[[<span class="number">0.</span>    <span class="number">1.807</span> <span class="number">0.</span>    <span class="number">0.</span>    <span class="number">0.</span>    <span class="number">0.</span>    <span class="number">0.</span>   ]</span><br><span class="line"> [<span class="number">1.807</span> <span class="number">0.</span>    <span class="number">0.807</span> <span class="number">0.</span>    <span class="number">0.807</span> <span class="number">0.807</span> <span class="number">0.</span>   ]</span><br><span class="line"> [<span class="number">0.</span>    <span class="number">0.807</span> <span class="number">0.</span>    <span class="number">1.807</span> <span class="number">0.</span>    <span class="number">0.</span>    <span class="number">0.</span>   ]</span><br><span class="line"> [<span class="number">0.</span>    <span class="number">0.</span>    <span class="number">1.807</span> <span class="number">0.</span>    <span class="number">1.807</span> <span class="number">0.</span>    <span class="number">0.</span>   ]</span><br><span class="line"> [<span class="number">0.</span>    <span class="number">0.807</span> <span class="number">0.</span>    <span class="number">1.807</span> <span class="number">0.</span>    <span class="number">0.</span>    <span class="number">0.</span>   ]</span><br><span class="line"> [<span class="number">0.</span>    <span class="number">0.807</span> <span class="number">0.</span>    <span class="number">0.</span>    <span class="number">0.</span>    <span class="number">0.</span>    <span class="number">2.807</span>]</span><br><span class="line"> [<span class="number">0.</span>    <span class="number">0.</span>    <span class="number">0.</span>    <span class="number">0.</span>    <span class="number">0.</span>    <span class="number">2.807</span> <span class="number">0.</span>   ]]</span><br></pre></td></tr></table></figure>
<p></p>

<p>이것으로 동시발생 행렬을 PPMI 행렬로 변환하는 법을 알아봤다. 이때 PPMI 행렬의 각 원소는 0 이상의 실수이다. 이제 우리는 더 좋은 척도로 이뤄진 행렬(더 좋은 단어 벡터)을 손에 쥐었다. 그러나 PPMI 행렬에도 여전히 큰 문제가 있다! 말뭉치의 어휘 수가 증가함에 따라 각 단어 벡터의 차원 수도 증가한다는 문제다. 예를 들어 말뭉치의 어휘 수가 10만 개라면 그 벡터의 차원 수도 똑같이 10만이 된다. 10만 차원의 벡터를 다룬다는 것은 그다지 현실적이지 않다.</p>
<p></p>

<p>또한, 이 행렬의 내용을 들여다보면 원소 대부분이 0인 것을 알 수 있다. 벡터의 원소 대부분이 중요하지 않다는 뜻이다. 다르게 표현하면 각 원소의 중요도가 낮다는 뜻이다. 더구나 이런 벡터는 노이즈에 약하고 견고하지 못하다는 약점도 있다. 이 문제에 대처하고자 자주 수행하는 기법이 바로 벡터의 차원 감소이다.</p>
<p><br></br><br></p>
<h4 id="차원-감소"><a href="#차원-감소" class="headerlink" title="차원 감소"></a>차원 감소</h4><p>차원 감소는 문자 그대로 벡터의 차원을 줄이는 방법을 말한다. 그러나 단순히 줄이기만 하는 게 아니라, ‘중요한 정보’는 최대한 유지하면서 줄이는 게 핵심이다. 직관적인 예로, 아래 그림처럼 데이터의 분포를 고려해 중요한 ‘축’을 찾는 일을 수행한다.</p>
<p></p>

<p><img src="/image/fnlp4.jpeg" alt="fnlp4"></p>
<p></p>

<p>위 그림에서 왼쪽은 데이터점들을 2차원 좌표에 표시한 모습이다. 그리고 오른쪽은 새로운 축을 도입하여 똑같은 데이터를 좌표축 하나만으로 표시했다(새로운 축을 찾을 때는 데이터가 넓게 분포되도록 고려해야 한다). 이때 각 데이터점의 값은 새로운 축으로 사영된 값으로 변한다. 여기서 중요한 것은 가장 적합한 축을 찾아내는 일로, 1차원 값만으로도 데이터의 본질적인 차이를 구별할 수 있어야 한다. 이와 같은 작업은 다차원 데이터에 대해서도 수행할 수 있다.</p>
<p></p>

<p>원소 대부분이 0인 행렬 또는 벡터를 희소행렬 또는 희소벡터라 한다. 차원 감소의 핵심은 희소벡터에서 중요한 축을 찾아내어 더 적은 차원으로 다시 표현하는 것인데, 차원 감소의 결과로 원래으 희소벡터는 원소 대부분이 0이 아닌 값으로 구성된 밀집벡터로 변환된다. 이 조밀한 벡터야말로 우리가 원하는 단어의 분산 표현이다.</p>
<p></p>

<p>차원을 감소시키는 방법은 여러 가지가 있지만, 우리는 특잇값분해(Singular Value Decomposition, SVD)를 이용하겠다. SVD는 임의의 행렬을 세 행렬의 곱으로 분해하며, 수식으로는 다음과 같다.</p>
<p></p>

<script type="math/tex; mode=display">X = USV^T</script><p></p>

<p>위의 식과 같이 SVD는 임의의 행렬 $\vec{X}$를 $\vec{U, S, V}$라는 세 행렬의 곱으로 분해한다. 여기서 $\vec{U, V}$는 직교행렬이고, 그 열벡터는 서로 직교한다. 또한 $\vec{S}$는 대각행렬(대각성분 외에는 모두 0인 행렬)이다. 이 수식을 시각적으로 표현하면 아래와 같다.</p>
<p></p>

<p><img src="/image/fnlp5.jpeg" alt="fnlp5"></p>
<p></p>

<p>자, $\vec{U}$는 직교행렬이다. 그리고 이 직교행렬은 어떠한 공간의 축(기저)을 형성한다. 지금 우리의 맥락에서는 이 $\vec{U}$ 행렬을 ‘단어 공간’으로 취급할 수 있다. 또한 $\vec{S}$는 대각행렬로, 그 대각성분에는 ‘특잇값(singular value)’이 큰 순서로 나열되어 있다. 특잇값이란, 쉽게 말해 ‘해당 축’의 중요도라고 간주할 수 있다. 그래서 아래 그림과 같이 중요도가 낮은 원소(특잇값이 작은 원소)를 깎아내는 방법을 생각할 수 있다.</p>
<p></p>

<p><img src="/image/fnlp6.jpeg" alt="fnlp6"></p>
<p></p>

<p>위 그림을 잘 보면서 다음 설명을 따라가보자. 행렬 $\vec(S)$에서 특잇값이 작다면 중요도가 낮다는 뜻이므로, 행렬 $\vec(U)$에서 여분의 열벡터를 깎아내어 원래의 행렬을 근사할 수 있다. 이를 우리 문제로 가져와서 ‘단어의 PPMI 행렬’에 적용해볼까? 그러면 행렬 $\vec{X}$의 각 행에는 해당 단어 ID의 단어 벡터가 저장되어 있으며, 그 단어 벡터가 행렬 $\vec{U’}$라는 차원 감소된 벡터로 표현되는 것이다.</p>
<p><br></br><br></p>
<h4 id="SVD에-의한-차원-감소"><a href="#SVD에-의한-차원-감소" class="headerlink" title="SVD에 의한 차원 감소"></a>SVD에 의한 차원 감소</h4><p>이제 SVD를 파이썬 코드로 살펴보자. SVD는 넘파이의 linalg 모듈이 제공하는 svd 메서드로 실행할 수 있다. 그럼 동시발생 행렬을 만들어 PPMI 행렬로 변환한 다음 SVD를 적용해보자.</p>
<p></p>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># SVD</span></span><br><span class="line">U, S, V = np.linalg.svd(W)</span><br><span class="line"></span><br><span class="line">print(C[<span class="number">0</span>])  <span class="comment"># 동시발생 행렬</span></span><br><span class="line">print(W[<span class="number">0</span>])  <span class="comment"># PPMI 행렬</span></span><br><span class="line">print(U[<span class="number">0</span>])  <span class="comment"># SVD</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 실행결과</span></span><br><span class="line">[<span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">[<span class="number">0.</span>    <span class="number">1.807</span> <span class="number">0.</span>    <span class="number">0.</span>    <span class="number">0.</span>    <span class="number">0.</span>    <span class="number">0.</span>   ]</span><br><span class="line">[ <span class="number">3.409e-01</span>  <span class="number">0.000e+00</span> <span class="number">-1.205e-01</span> <span class="number">-3.886e-16</span> <span class="number">-9.323e-01</span> <span class="number">-1.110e-16</span></span><br><span class="line"> <span class="number">-2.426e-17</span>]</span><br></pre></td></tr></table></figure>
<p></p>

<p>이제 SVD를 수행해 보면 위 코드에서 SVD에 의해 변환된 밀집벡터 표현은 변수 U에 저장된다. 단어 ID가 0인 단어 벡터를 보면 원래는 희소벡터인 W[0]가 SVD에 의해서 U[0]로 변했다. 그리고 이 밀집벡터의 차원을 감소시키려면, 예컨대 2차원 벡터로 줄이려면 단순히 처음의 두 원소를 꺼내면 된다.</p>
<p></p>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">print(U[<span class="number">0</span>, :<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 실행결과</span></span><br><span class="line">[<span class="number">0.341</span> <span class="number">0.</span>   ]</span><br></pre></td></tr></table></figure>
<p></p>

<p>이것으로 차원 감소도 다 알아봤다. 그러면 각 단어를 2차원 벡터로 표현한 후 그래프로 그려보자. 다음 코드를 추가하면 된다.</p>
<p></p>

<p><img src="/image/myplot.png" alt="myplot"></p>
<p></p>

<p>위 그림을 보면 goodbye와 hello, you와 i가 제법 가까이 있음을 알 수 있다. 우리의 직관과 비교적 비슷하다. 하지만 지금 사용한 말뭉치가 아주 작아서 이 결과를 그대로 받아들이기에는 솔직히 석연치 않다. 그러면 계속해서 PTB 데이터셋이라는 더 큰 말뭉치를 사용하여 똑같은 실험을 수행해보자.</p>
<p><br></br><br></p>
<h4 id="PBT-데이터셋"><a href="#PBT-데이터셋" class="headerlink" title="PBT 데이터셋"></a>PBT 데이터셋</h4><p>지금까지는 아주 작은 텍스트 데이터를 말뭉치로 사용했다. 그래서 이번 절에서는 본격적인 말뭉치(그렇다고 너무 크지는 않고 적당한 말뭉치)를 이용해보겠다. 그 주인공은 바로 펜 트리뱅크(Penn Treebank, PTB)이다. PTB 말뭉치는 주어진 기법의 품질을 측정하는 벤치마크로 자주 이용된다. 우리가 이용할 PTB 말뭉치는 word2vec 발명자인 토마스 마콜로프의 웹 페이지에서 받을 수 있다. 이 PTB 말뭉치는 텍스트 파일로 제공되며, 원래의 PTB 문장에 몇 가지 전처리를 해두었다. 예컨대 희소한 단어를 <unk>라는 특수 문자로 치환한다거나, 구체적인 숫자를 ‘N’으로 대체하는 등의 작업이 적용되었다. 또한 PTB 말뭉치에서는 한 문장이 하나의 줄로 저장되어 있다. 이 책에서는 각 문장을 연결한 ‘하나의 큰 시계열 데이터’로 취급한다. 이때 각 문장 끝에 <eos>라는 특수 문자를 삽입한다. </p>
<p></p>

<p>이 책에서는 PTB 데이터셋을 쉽게 이용할 수 있도록 전용 파이썬 코드를 준비했다. 다음은 ptb.py를 사용하는 예이다.</p>
<p></p>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">'..'</span>)</span><br><span class="line"><span class="keyword">import</span> ptb</span><br><span class="line"></span><br><span class="line">corpus, word_to_id, id_to_word = ptb.load_data(<span class="string">'train'</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'말뭉치 크기:'</span>, len(corpus))</span><br><span class="line">print(<span class="string">'corpus[:30]:'</span>, corpus[:<span class="number">30</span>])</span><br><span class="line">print()</span><br><span class="line">print(<span class="string">'id_to_word[0]:'</span>, id_to_word[<span class="number">0</span>])</span><br><span class="line">print(<span class="string">'id_to_word[0]:'</span>, id_to_word[<span class="number">1</span>])</span><br><span class="line">print(<span class="string">'id_to_word[0]:'</span>, id_to_word[<span class="number">2</span>])</span><br><span class="line">print()</span><br><span class="line">print(<span class="string">'word_to_id["car"]:'</span>, word_to_id[<span class="string">"car"</span>])</span><br><span class="line">print(<span class="string">'word_to_id["happy"]:'</span>, word_to_id[<span class="string">"happy"</span>])</span><br><span class="line">print(<span class="string">'word_to_id["lexus"]:'</span>, word_to_id[<span class="string">"lexus"</span>])</span><br></pre></td></tr></table></figure>
<p></p>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 실행결과</span></span><br><span class="line"></span><br><span class="line">말뭉치 크기: <span class="number">929589</span></span><br><span class="line">corpus[:<span class="number">30</span>]: [ <span class="number">0</span>  <span class="number">1</span>  <span class="number">2</span>  <span class="number">3</span>  <span class="number">4</span>  <span class="number">5</span>  <span class="number">6</span>  <span class="number">7</span>  <span class="number">8</span>  <span class="number">9</span> <span class="number">10</span> <span class="number">11</span> <span class="number">12</span> <span class="number">13</span> <span class="number">14</span> <span class="number">15</span> <span class="number">16</span> <span class="number">17</span> <span class="number">18</span> <span class="number">19</span> <span class="number">20</span> <span class="number">21</span> <span class="number">22</span> <span class="number">23</span></span><br><span class="line"> <span class="number">24</span> <span class="number">25</span> <span class="number">26</span> <span class="number">27</span> <span class="number">28</span> <span class="number">29</span>]</span><br><span class="line"></span><br><span class="line">id_to_word[<span class="number">0</span>]: aer</span><br><span class="line">id_to_word[<span class="number">0</span>]: banknote</span><br><span class="line">id_to_word[<span class="number">0</span>]: berlitz</span><br><span class="line"></span><br><span class="line">word_to_id[<span class="string">"car"</span>]: <span class="number">3856</span></span><br><span class="line">word_to_id[<span class="string">"happy"</span>]: <span class="number">4428</span></span><br><span class="line">word_to_id[<span class="string">"lexus"</span>]: <span class="number">7426</span></span><br><span class="line">``` </span><br><span class="line"></span><br><span class="line">&lt;p&gt;&lt;/p&gt;</span><br><span class="line"></span><br><span class="line">말뭉치를 다루는 방법은 지금까지와 같다. 앞의 코드에서 ptb.load_data()는 데이터를 읽어 들인다. 이때 인수로는 <span class="string">'train'</span>, <span class="string">'test'</span>, <span class="string">'valid'</span> 중 하나를 지정할 수 있는데, 차례대로 훈련용, 테스트용, 검증용 데이터를 가리킨다. 이상으로 ptb 사용법에 관한 설명을 마친다.</span><br><span class="line"></span><br><span class="line">&lt;br&gt;&lt;/br&gt;&lt;br&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment">#### PTB 데이터셋 평가</span></span><br><span class="line"></span><br><span class="line">PTB 데이터셋에 통계 기반 기법을 적용해보자. 이번에는 큰 행렬에 SVD를 적용해야 하므로 고속 SVD를 이용할 것을 추천한다. 구속 SVD를 이용하려면 sklearn 모듈을 설치해야 한다.</span><br><span class="line"></span><br><span class="line">&lt;p&gt;&lt;/p&gt;</span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">'..'</span>)</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> util <span class="keyword">import</span> most_similar, create_co_matrix, ppmi</span><br><span class="line"><span class="keyword">import</span> ptb</span><br><span class="line"></span><br><span class="line">window_size = <span class="number">2</span></span><br><span class="line">wordvec_size = <span class="number">100</span></span><br><span class="line"></span><br><span class="line">corpus, word_to_id, id_to_word = ptb.load_data(<span class="string">'train'</span>)</span><br><span class="line">vocab_size = len(word_to_id)</span><br><span class="line">print(<span class="string">'동시발생 수 계산 ...'</span>)</span><br><span class="line">C = create_co_matrix(corpus, vocab_size, window_size)</span><br><span class="line">print(<span class="string">'PPMI 계산 ...'</span>)</span><br><span class="line">W = ppmi(C, verbose=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'SVD 계산 ...'</span>)</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="comment"># truncated SVD (빠르다!)</span></span><br><span class="line">    <span class="keyword">from</span> sklearn.utils.extmath <span class="keyword">import</span> randomized_svd</span><br><span class="line">    U, S, V = randomized_svd(W, n_components=wordvec_size, n_iter=<span class="number">5</span>,</span><br><span class="line">                             random_state=<span class="literal">None</span>)</span><br><span class="line"><span class="keyword">except</span> ImportError:</span><br><span class="line">    <span class="comment"># SVD (느리다)</span></span><br><span class="line">    U, S, V = np.linalg.svd(W)</span><br><span class="line"></span><br><span class="line">word_vecs = U[:, :wordvec_size]</span><br><span class="line"></span><br><span class="line">querys = [<span class="string">'you'</span>, <span class="string">'year'</span>, <span class="string">'car'</span>, <span class="string">'toyota'</span>]</span><br><span class="line"><span class="keyword">for</span> query <span class="keyword">in</span> querys:</span><br><span class="line">    most_similar(query, word_to_id, id_to_word, word_vecs, top=<span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<p></p>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 실행결과</span></span><br><span class="line"></span><br><span class="line">말뭉치 크기: <span class="number">929589</span></span><br><span class="line">corpus[:<span class="number">30</span>]: [ <span class="number">0</span>  <span class="number">1</span>  <span class="number">2</span>  <span class="number">3</span>  <span class="number">4</span>  <span class="number">5</span>  <span class="number">6</span>  <span class="number">7</span>  <span class="number">8</span>  <span class="number">9</span> <span class="number">10</span> <span class="number">11</span> <span class="number">12</span> <span class="number">13</span> <span class="number">14</span> <span class="number">15</span> <span class="number">16</span> <span class="number">17</span> <span class="number">18</span> <span class="number">19</span> <span class="number">20</span> <span class="number">21</span> <span class="number">22</span> <span class="number">23</span></span><br><span class="line"> <span class="number">24</span> <span class="number">25</span> <span class="number">26</span> <span class="number">27</span> <span class="number">28</span> <span class="number">29</span>]</span><br><span class="line"></span><br><span class="line">id_to_word[<span class="number">0</span>]: aer</span><br><span class="line">id_to_word[<span class="number">0</span>]: banknote</span><br><span class="line">id_to_word[<span class="number">0</span>]: berlitz</span><br><span class="line"></span><br><span class="line">word_to_id[<span class="string">"car"</span>]: <span class="number">3856</span></span><br><span class="line">word_to_id[<span class="string">"happy"</span>]: <span class="number">4428</span></span><br><span class="line">word_to_id[<span class="string">"lexus"</span>]: <span class="number">7426</span></span><br></pre></td></tr></table></figure>
<p></p>

<p>위 코드는 SVD를 수행하는데 sklearn_randomized_svd() 메서드를 이용했다. 이 메서드는 무작위 수를 사용한 Truncated SVD로, 특잇값이 큰 것들만 계산하여 기본적인 SVD보다 훨씬 빠르다. 나머지 부분은 앞서 작은 말뭉치를 사용한 코드와 거의 같다. 위 코드를 실행결과를 보면, 우선 you라는 검색어에서는 인칭대명사인 i와 we가 상위를 차지했음을 알 수 있다. 영어 문장에서 관용적으로 자주 같이 나오는 단어들이기 때문이다. 또 year의 연관어로는 month와 quarter가, car의 연관어로는 auto와 vehicle 등이 뽑혔다. 그리고 toyota와 관련된 단어로는 nissan, honda, lexus 등 자동차 제조업체나 브랜드가 뽑힌 것도 확인할 수 있다. 이처럼 단어의 의미 혹은 문법적인 관점에서 비슷한 단어들이 가까운 벡터로 나타났다. 우리의 직관과 비슷한 결과라고 할 수 있다.</p>
<p></p>

<p>마침내 단어이 의미를 벡터로 잘 인코딩하는데 성공했다. 말뭉치를 사용해 맥락에 속한 단어의 등장 횟수를 센 후 PPMI 행렬로 변환하고, 다시 SVD를 이용해 차원을 감소시킴으로써 더 좋은 단어 벡터를 얻어냈다. 이것이 단어의 분산 표현이고, 각 단어는 고정 길이의 밀집벡터로 표현되었다.</p>

    </div>

    

    
        <div class="post-tags">
            <i class="fa fa-tags" aria-hidden="true"></i>
            <a href="/tags/natural-language-processing/">#Natural Language Processing</a>
        </div>
    

    <!-- Comments -->
    

</div>
        </section>

    </div>
</div>


</div>

<!-- Footer -->
<div class="push"></div>

<footer class="footer-content">
    <div class="container">
        <div class="row">
            <div class="col-xs-12 col-sm-12 col-md-6 col-lg-6 footer-about">
                <h2>About</h2>
                <p>
                    This theme was developed by <a href="https://github.com/klugjo" target="_blank" rel="noopener">Jonathan Klughertz</a>. The source code is available on Github. Create Websites. Make Magic.
                </p>
            </div>
            
    <div class="col-xs-6 col-sm-6 col-md-3 col-lg-3 recent-posts">
        <h2>Recent Posts</h2>
        <ul>
            
            <li>
                <a class="footer-post" href="/2020/01/20/SEM-%EC%B8%A1%EC%A0%95%EB%AA%A8%ED%98%95/">[SEM] 측정모형</a>
            </li>
            
            <li>
                <a class="footer-post" href="/2020/01/15/edwith-%EC%9B%B9%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D%EA%B8%B0%EC%B4%88/">[edwith] 웹 프로그래밍 기초</a>
            </li>
            
            <li>
                <a class="footer-post" href="/2020/01/14/fnlp-%EB%B0%91%EB%B0%94%EB%8B%A5%EB%B6%80%ED%84%B0-%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94-%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC-1/">[FNLP] 밑바닥부터 시작하는 자연어처리 (</a>
            </li>
            
            <li>
                <a class="footer-post" href="/2019/12/25/SEM-%EC%B8%A1%EC%A0%95%EB%AA%A8%ED%98%95%EC%9D%98-%ED%99%95%EC%9E%A5/">[SEM] 측정모형의 확장</a>
            </li>
            
        </ul>
    </div>



            
<div class="col-xs-6 col-sm-6 col-md-3 col-lg-3 footer-categories">
    <h2>Categories</h2>
    <ul>
        
        <li>
            <a class="footer-post" href="/categories/algorithm/">algorithm</a>
        </li>
        
        <li>
            <a class="footer-post" href="/categories/algorithm/selection-sort/">selection sort</a>
        </li>
        
        <li>
            <a class="footer-post" href="/categories/natural-language-processing/">Natural Language Processi</a>
        </li>
        
        <li>
            <a class="footer-post" href="/categories/vuejs/">vuejs</a>
        </li>
        
    </ul>
</div>

        </div>
        <div class="row">
            <div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">
                <ul class="list-inline footer-social-icons">
                    
                    <li class="list-inline-item">
                        <a href="https://github.com/klugjo/hexo-theme-alpha-dust" target="_blank" rel="noopener">
                            <span class="footer-icon-container">
                                <i class="fa fa-github"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="https://twitter.com/?lang=en" target="_blank" rel="noopener">
                            <span class="footer-icon-container">
                                <i class="fa fa-twitter"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="https://www.facebook.com/" target="_blank" rel="noopener">
                            <span class="footer-icon-container">
                                <i class="fa fa-facebook"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="https://www.instagram.com/" target="_blank" rel="noopener">
                            <span class="footer-icon-container">
                                <i class="fa fa-instagram"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="https://dribbble.com/" target="_blank" rel="noopener">
                            <span class="footer-icon-container">
                                <i class="fa fa-dribbble"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="https://plus.google.com/" target="_blank" rel="noopener">
                            <span class="footer-icon-container">
                                <i class="fa fa-google-plus"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="https://www.behance.net/" target="_blank" rel="noopener">
                            <span class="footer-icon-container">
                                <i class="fa fa-behance"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="https://500px.com/" target="_blank" rel="noopener">
                            <span class="footer-icon-container">
                                <i class="fa fa-500px"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="mailto:test@example.com" target="_blank" rel="noopener">
                            <span class="footer-icon-container">
                                <i class="fa fa-envelope-o"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="\#">
                            <span class="footer-icon-container">
                                <i class="fa fa-rss"></i>
                            </span>
                        </a>
                    </li>
                    
                </ul>
            </div>
        </div>
        <div class="row">
            <div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">
                <div class="footer-copyright">
                    @Untitled. All right reserved | Design & Hexo <a href="http://www.codeblocq.com/" target="_blank" rel="noopener">Jonathan Klughertz</a>
                </div>
            </div>
        </div>
    </div>
</footer>

<!-- After footer scripts -->

<!-- jQuery -->
<script src="//code.jquery.com/jquery-2.1.4.min.js"></script>

<!-- Tween Max -->
<script src="//cdnjs.cloudflare.com/ajax/libs/gsap/1.18.5/TweenMax.min.js"></script>

<!-- Gallery -->
<script src="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.js" type="text/javascript" charset="utf-8"></script>

<!-- Custom JavaScript -->
<script src="/js/main.js"></script>

<!-- Disqus Comments -->



<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML'></script>

</body>

</html>